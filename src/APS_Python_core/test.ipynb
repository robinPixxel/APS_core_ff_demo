{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# with open('1_input_data/config.json', 'r') as file:\n",
    "#         config = json.load(file)\n",
    "# image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "\n",
    "# image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "# image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "# image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "# image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "# image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "# image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "# base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "\n",
    "# image_opportunity_df['base_time'] = base_time_stamp\n",
    "# image_opportunity_df['req_date'] = image_opportunity_df[['base_time','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)#pd.to_datetime(image_opportunity_df['base_time']) + pd.to_timedelta(image_opportunity_df['OpportunityStartOffset'])\n",
    "# image_opportunity_df[['req_date','OpportunityStartTime','base_time','OpportunityStartOffset']]\n",
    "\n",
    "# image_opportunity_df['x'] = image_opportunity_df[['Priority','StripID']].apply(lambda a: a['Priority'] if a['Priority']<=0 else a['StripID'],axis=1)\n",
    "# image_opportunity_df[['Priority','StripID','x']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa']\n",
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa', '../']\n"
     ]
    }
   ],
   "source": [
    "#!pip list\n",
    "import os \n",
    "os.getcwd()\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 66, 'c': 43, 'd': 55, 'ww': 222}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_opportunity_df.columns\n",
    "result_dict = {'a':2,'b':66,'c':43}\n",
    "result_dict.update({'d':55,'ww':222})\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "# from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "# from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "# from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "# from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "# from model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "# from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "# from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "# from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "# from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "# from result_interpret import interpret_result\n",
    "# from utils import *\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from APS_Python_core.model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from APS_Python_core.postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from APS_Python_core.model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "from APS_Python_core.postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from APS_Python_core.model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from APS_Python_core.postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from APS_Python_core.result_interpret import interpret_result\n",
    "from APS_Python_core.utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['SG1K1G2K2_pair']['domain_of_csgk'])\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_time'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    # data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    #GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df_original = GS_pass_df.copy()\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    #image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    #image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.6))] + [0 for i in range(int(1.5*3600*0.4))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = GS_pass_df_original\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "def get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    #APS_Python_core/src/APS_Python_core/1_input_data/config.json\n",
    "    #with open('APS_Python_core/src/APS_Python_core/1_input_data/config.json', 'r') as file:\n",
    "    #with open('../1_input_data/config.json', 'r') as file:\n",
    "        #config = json.load(file)\n",
    "    original_image_opportunity_df = image_opportunity_df.copy()\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "    config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "    config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    # gs_pass_result_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    # img_capture_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    # APS_success_metric_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/APS_success_metric.csv\",index = None)\n",
    "    # downlink_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = original_image_opportunity_df\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        #v.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/\"+k+\".csv\",index = None)\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_gsPass_result = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout'][['SatID','start_time','end_time','gsID','base_time']]\n",
    "    only_gsPass_result['start_time'] = only_gsPass_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_gsPass_result['end_time'] = only_gsPass_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result,\\\n",
    "                  \"only_gsPass_result\":only_gsPass_result,\\\n",
    "                  \"combined_result\":img_capture_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "    #return result_dict\n",
    "    return result_dict\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_solving\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [1e+00, 1e+09]\n",
      "  Cost   [1e+00, 1e+00]\n",
      "  Bound  [1e+00, 1e+00]\n",
      "  RHS    [9e+04, 1e+09]\n",
      "Presolving model\n",
      "335 rows, 250 cols, 750 nonzeros  0s\n",
      "169 rows, 167 cols, 418 nonzeros  0s\n",
      "77 rows, 75 cols, 188 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   77 rows\n",
      "   75 cols (37 binary, 0 integer, 0 implied int., 38 continuous)\n",
      "   188 nonzeros\n",
      "MIP-Timing:     0.00064 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -inf            inf                  inf        0      0      0         0     0.0s\n",
      " R       0       0         0   0.00%   -3001279.839525 -34044          8715.88%        0      0      0        76     0.0s\n",
      " C       0       0         0   0.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "         1       0         1 100.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -45431\n",
      "  Dual bound        -45431\n",
      "  Gap               0% (tolerance: 0.01%)\n",
      "  P-D integral      0.0354302050283\n",
      "  Solution status   feasible\n",
      "                    -45431 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    0 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            0.00 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 0\n",
      "  Nodes             1\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     152 (total)\n",
      "                    0 (strong br.)\n",
      "                    76 (separation)\n",
      "                    0 (heuristics)\n",
      "status= Optimal\n",
      "image_capture_plan_starting\n",
      "=====IMAGING THEMAL======\n",
      "=====READOUT THERMAL======\n",
      "len_before_eclipse_transition_divide= 437\n",
      "len_after(after explode)_eclipse_transition_divide= 517\n",
      "Before_Ambiguous_event_transition_divide = 517\n",
      "Empty DataFrame\n",
      "Columns: [SatID, encoded_stripId, start_time, end_time, Eclipse, TW_index, gsID, till_now_max, prev_max, global_TW, Memory_global_TW_index, concat_sat_MGWI, EcStEnd_list, len_EcStEnd_list, new_eclipse, new_start_time, new_end_time]\n",
      "Index: []\n",
      "After_len_of_ambiguous_event_transition_divide = 517\n",
      "list of unique lenths of eclipse_transition_divide(bifurcation) =  [1 2 3]\n",
      "len_power_based_memory_based= 372 len_power_based_memory_based= 145\n",
      "final_len_power_based= 517\n",
      "GS_Pass_time_objective\n",
      "only gs check  FF01 1.0 [1.0]\n",
      "only gs check  FF02 1.0 [1.0]\n",
      "only gs check  FF03 1.0 [1.0]\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [8e-01, 1e+06]\n",
      "  Cost   [1e+00, 7e+02]\n",
      "  Bound  [1e+00, 7e+08]\n",
      "  RHS    [1e+00, 2e+08]\n",
      "Presolving model\n",
      "14869 rows, 4981 cols, 42733 nonzeros  0s\n",
      "8647 rows, 4364 cols, 26526 nonzeros  0s\n",
      "8647 rows, 4364 cols, 23724 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   8647 rows\n",
      "   4364 cols (3155 binary, 0 integer, 0 implied int., 1209 continuous)\n",
      "   23724 nonzeros\n",
      "MIP-Timing:       0.037 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -141310.402767  inf                  inf        0      0      0         0     0.0s\n",
      "         0       0         0   0.00%   -91644.261909   inf                  inf        0      0      3      4356     0.1s\n",
      " C       0       0         0   0.00%   -86574.985614   -35885.942577    141.25%       89     60     20      4602     0.4s\n",
      " L       0       0         0   0.00%   -65097.424698   -45125.372031     44.26%      766    273     20      8290     2.2s\n",
      " L       0       0         0   0.00%   -65097.424698   -45814.114173     42.09%      720    203     20      9134     2.6s\n",
      " T       0       0         0   0.00%   -65097.424698   -47536.654582     36.94%      720    203     20      9900     3.1s\n",
      " L     181     175         1   0.00%   -65097.424698   -47610.97665      36.73%     1746    361     20     12339     4.0s\n",
      " L     462     453         2   0.00%   -64612.738167   -47791.543184     35.20%     1768    234     39     14421     5.0s\n",
      " L     667     644         9   0.00%   -62378.92478    -47798.633484     30.50%     1787    239    244     16661     6.2s\n",
      "      1089     850        98   0.13%   -61939.761571   -47798.633484     29.58%     2133    198   2841     43044    11.7s\n",
      " L    1200     855       140   0.15%   -61719.337809   -47822.737015     29.06%     2181    251   4196     46951    12.4s\n",
      " L    1312     852       187   0.18%   -61649.218266   -48004.215293     28.42%     1776    320   5440     54364    13.9s\n",
      " L    1417     844       218   0.29%   -61555.479498   -48283.582431     27.49%     1780    209   5999     57632    14.9s\n",
      "      1658     864       318   0.48%   -60549.163653   -48283.582431     25.40%     1759     81   8510     88393    20.1s\n",
      "      2222     904       555   0.70%   -60341.999609   -48283.582431     24.97%     1882    276   9666    115522    25.3s\n",
      "      2456     923       657   0.73%   -60341.999609   -48283.582431     24.97%     1778     79   9963    143994    30.5s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 4434 rows, 1559 cols (353 bin., 0 int., 0 impl., 1206 cont.), and 12498 nonzeros\n",
      "\n",
      "      2717       0         0   0.00%   -49179.53302    -48283.582431      1.86%      173      0      0    159666    33.5s\n",
      "      2717       0         0   0.00%   -49179.53302    -48283.582431      1.86%      173    109      4    160180    33.5s\n",
      "      2718       0         1 100.00%   -48283.582431   -48283.582431      0.00%      299    122      4    160225    33.5s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -48283.5824309\n",
      "  Dual bound        -48283.5824309\n",
      "  Gap               0%\n",
      "  P-D integral      10.5774928693\n",
      "  Solution status   feasible\n",
      "                    -48283.5824309 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    0 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            33.54 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 5\n",
      "  Nodes             2718\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     160225 (total)\n",
      "                    0 (strong br.)\n",
      "                    10192 (separation)\n",
      "                    92454 (heuristics)\n",
      "status= Optimal\n",
      "Downlink_plan_starting\n"
     ]
    }
   ],
   "source": [
    "GS_pass_df = GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "image_opportunity_df = pd.read_csv(\"1_input_data/Imaging_new (1) copy.csv\")\n",
    "image_downlink_df = pd.read_csv(\"1_input_data/APS_imageTable_TV1.csv\")\n",
    "with open('1_input_data/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "result_dict = get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n",
    "for k,v in result_dict.items():\n",
    "    v.to_csv('5_output_data/'+k+'.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_pass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(image_opportunity_df['Priority'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 83)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['only_img_capture_result']['StripID'].nunique(),len(result_dict['only_gsPass_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['only_gsPass_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture_plan_data_input['']\n",
    "#-48283.5824309\n",
    "#-48271\n",
    "len(result_dict['interpret_selected_oppr_conflict_comparision_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['interpret_selected_oppr_conflict_comparision_df']#['conflic_strip_flag_named'].nunique()##['interpret_selected_oppr_conflict_comparision_df']#['interpret_extracted_raw_file_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlink_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 'extracted_raw_file_df', 'selected_oppr_conflict_comparision_df', 'KPI_df'\n",
    "df111 = result_dict['extracted_raw_file_df']#.columns\n",
    "#interpret_result_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = img_capture_result[img_capture_result['operation']=='Imaging']#['StripID']#.nunique()\n",
    "Z = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout']\n",
    "Z['duration'] = Z['end_time']- Z['start_time']\n",
    "#Y[Y['StripID']=='']\n",
    "\n",
    "#Y[Y['encoded_strip_id']=='Order 1 - Strip 0_Area 0']\n",
    "Y['StripID'].nunique(),len(Y),len(Z),Z['duration'].sum(),gs_pass_result_df['duration'].sum(),len(gs_pass_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['SatID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = Y[Y['SatID']=='FF03']\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "CR = RZO[RZO['SatID']=='FF03']\n",
    "pd.concat([CI,CR]).sort_values(by='start_time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Order 1 - Strip 0_Area 0'\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "RZO[RZO['SatID']=='FF01']\n",
    "RZO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RZ = capture_plan_data_input['dedicated_readout_df']\n",
    "len(RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF02'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF03'].sort_values(by='start_time'))\n",
    "RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_capture_result['operation'].unique()\n",
    "\n",
    "[ item for item in capture_plan_data_input['Memory_NoimageGs_TW_list'] if item[2]=='FF01']\n",
    "#capture_plan_data_input['dedicatedReadoutTWlist__concat_sat_memoryTWindex']['FF01_136.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_eclipse_data.csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eclipse_df[eclipse_df['eclipse']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_df.sort_values(by='time_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SatID,start_time,end_time,eclipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['GS_pass_df', 'image_opportunity_df', 'image_downlink_df', 'eclipse_df_dict', 'config']\n",
    "#input_dict['eclipse_df_dict']['FF02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['readout_memory_capacity__s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['imgery_sat_id_list']#.keys()\n",
    "[s+'_'+str(n) for s in capture_plan_data_input['imgery_sat_id_list']+capture_plan_data_input['only_gs_sat_id_list'] \\\n",
    " if s in capture_plan_data_input['dedicatedReadoutTWIndex__sat'].keys() for n in capture_plan_data_input['dedicatedReadoutTWIndex__sat'][s]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "\n",
    "#eclipse_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['assured_tasking_based_on_input_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['cs1j2k2Domainlist__cs1j1k1']['FF01_Order 1 - Strip 0_Area 0_1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['GS_Pass_time_objective'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(img_capture_result[img_capture_result['operation']=='Imaging'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "memory_plot_df = result_dict['combined_result']\n",
    "\n",
    "#memory_plot_df.columns\n",
    "#memory_plot_df[['SatID','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','base_time','OpportunityStartOffset','OpportunityEndOffset#']]\n",
    "memory_plot_df = memory_plot_df[['SatID','start_time','end_time','operation','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','base_time']]\n",
    "memory_plot_df.sort_values(by=['SatID','start_time'],inplace=True)\n",
    "\n",
    "memory_plot_df['till_now_max'] = memory_plot_df.groupby('SatID')['end_time'].cummax()\n",
    "memory_plot_df['prev_max'] = memory_plot_df.groupby('SatID')['till_now_max'].shift(1)\n",
    "\n",
    "memory_plot_df1 = memory_plot_df[memory_plot_df['start_time'] > memory_plot_df['prev_max'] + 1] \n",
    "memory_plot_df1['start_time1'] = memory_plot_df1['prev_max'] + 1 #TODO1 +1 is okay ?\n",
    "memory_plot_df1['end_time1'] = memory_plot_df1['start_time'] - 1\n",
    "memory_plot_df1['operation'] = 'idle'\n",
    "\n",
    "memory_plot_df1 = memory_plot_df1[['SatID','start_time1','end_time1','operation','base_time']]\n",
    "\n",
    "#memory_plot_df1 = memory_plot_df1.drop(['start_time', 'end_time','till_now_max','prev_max'], axis=1)\n",
    "memory_plot_df1.rename(columns={'start_time1':'start_time','end_time1':'end_time'},inplace=True)\n",
    "#imgGS_union_df1 ==> contains TW without img and without gs pass  table without eclipse divide\n",
    "final_memory_plot_df = pd.concat([memory_plot_df,memory_plot_df1])\n",
    "final_memory_plot_df.sort_values(by=['SatID','start_time'],inplace=True)\n",
    "\n",
    "final_memory_plot_df['camera_memory_value_endofTW'] = final_memory_plot_df['camera_memory_value_endofTW'].ffill()\n",
    "final_memory_plot_df['delta_camera_memory_value_in_this_TW'] = final_memory_plot_df['delta_camera_memory_value_in_this_TW'].fillna(0)\n",
    "final_memory_plot_df['start_time'] = final_memory_plot_df[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "final_memory_plot_df['end_time'] = final_memory_plot_df[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "final_memory_plot_df = final_memory_plot_df[['SatID','start_time','end_time','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','operation','base_time']]\n",
    "sat_list = final_memory_plot_df['SatID'].unique()\n",
    "#sat_list = ['FF01']\n",
    "\n",
    "\n",
    "colors = {\n",
    "        \"Imaging\": \"blue\",\n",
    "        \"downlinking_from_Readout\": \"green\",\n",
    "        \"Readout\": \"red\",\n",
    "        \"idle\": \"gray\"\n",
    "    }\n",
    "\n",
    "\n",
    "for s in sat_list:\n",
    "    this_plot_df = final_memory_plot_df[final_memory_plot_df['SatID']==s]\n",
    "    start_time_list = this_plot_df['start_time'].to_list()\n",
    "    end_time_list = this_plot_df['end_time'].to_list()\n",
    "    operation_list = this_plot_df['operation'].to_list()\n",
    "\n",
    "    camera_memory_value_endofTW_list = this_plot_df['camera_memory_value_endofTW'].to_list()\n",
    "    this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_endofTW'].shift(1)\n",
    "\n",
    "    first_row_list = this_plot_df.values.tolist()[0]\n",
    "    first_row_operation = first_row_list[5]\n",
    "    first_row_memory_val = first_row_list[3]\n",
    "    ortherwise_memory_val =  first_row_list[3] - first_row_list[4]\n",
    "    if first_row_operation not in['Imaging','Readout']:\n",
    "        this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_startofTW_list'].fillna(first_row_memory_val)\n",
    "    else:\n",
    "        this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_startofTW_list'].fillna(ortherwise_memory_val)\n",
    "\n",
    "    camera_memory_value_StartofTW_list = this_plot_df['camera_memory_value_startofTW_list'].to_list()\n",
    "    \n",
    "    time_list = list(itertools.chain.from_iterable(zip(start_time_list,end_time_list )))\n",
    "    operation_list = list(itertools.chain.from_iterable(zip(operation_list,operation_list )))\n",
    "    camera_memory_value_list = list(itertools.chain.from_iterable(zip(camera_memory_value_StartofTW_list,camera_memory_value_endofTW_list )))\n",
    "    df = pd.DataFrame({'time':time_list,'operation':operation_list,'memory':camera_memory_value_list})\n",
    "\n",
    "    this_list = this_plot_df.values.tolist()\n",
    "    # Plot using Plotly Express\n",
    "    # fig = px.line(this_plot_df, x='start_time', y='camera_memory_value_endofTW', color='operation', line_group='operation',\n",
    "    #             title=\"Memory Usage of Different Operations\")\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #                         x=time_list,\n",
    "    #                         y=camera_memory_value_list,\n",
    "    #                         mode='lines+markers+text',\n",
    "    #                         #name=operation_list,\n",
    "    #                         #line=dict(color=colors.get(operation, 'black'), width=2),\n",
    "    #                         #text=[f\"({start_time}, {current_memory:.2f})\", f\"({end_time}, {end_memory:.2f})\"],\n",
    "    #                         #textposition=text_position\n",
    "    #                     ))\n",
    "    \n",
    "    # fig.show()\n",
    "    \n",
    "    for item in this_list:\n",
    "            start_time = item[1]\n",
    "            end_time = item[2]\n",
    "            current_memory = item[6]\n",
    "            end_memory = item[3]\n",
    "            operation = item[5]\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                            x=[start_time, end_time],\n",
    "                            y=[current_memory, end_memory],\n",
    "                            mode='lines+markers+text',\n",
    "                            name=operation+'_'+s,\n",
    "                            line=dict(color=colors.get(operation, 'black'), width=2),\n",
    "                            #text=[f\"({start_time}, {current_memory:.2f})\", f\"({end_time}, {end_memory:.2f})\"],\n",
    "                            #textposition=text_position\n",
    "                        ))\n",
    "\n",
    "    #     # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Memory Profile Over Time\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Memory\",\n",
    "        legend_title=\"Operations\",\n",
    "          )\n",
    "    # names = set()\n",
    "    # fig.for_each_trace(\n",
    "    #     lambda trace:\n",
    "    #         trace.update(showlegend=False)\n",
    "    #         if (trace.name in names) else names.add(trace.name))\n",
    "\n",
    "    # Save figure as HTML\n",
    "    html_filename = \"memory_profile_.html\"\n",
    "    fig.write_html('5_output_data'+'/'+html_filename)\n",
    "\n",
    "# final_memory_plot_df[final_memory_plot_df['SatID']=='FF01'].sort_values(by='start_time')\n",
    "\n",
    "import pygwalker as pyg\n",
    "pyg.walk(final_memory_plot_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import os\n",
    "from preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from result_interpret import interpret_result\n",
    "from utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_time'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    # data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 2  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = True\n",
    "    # #config['objective']['total_readout_memory'] = False\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    # data['total_priority_objective'] = obj_model.prob.objective.value()\n",
    "    #++++++++++++++++++++++++++  STEP 3  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    \n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.6))] + [0 for i in range(int(1.5*3600*0.4))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    with open('1_input_data/config.json', 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "        config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "        config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    try:\n",
    "        print(dfd)\n",
    "        downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    except:\n",
    "        print(\"downlink_schedule_has_some_error\")\n",
    "        downlink_result = pd.DataFrame()\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    gs_pass_result_df.to_csv(\"5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    img_capture_result.to_csv(\"5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    APS_success_metric_df.to_csv(\"5_output_data/APS_success_metric.csv\",index = None)\n",
    "    downlink_result.to_csv(\"5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        v.to_csv(\"5_output_data/\"+k+\".csv\",index = None)\n",
    "\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conflicting_dict(self,df,data_dict,different_setup_time,conflicting_on = 'GsID',concat_filter ='concat_gsid_satid_TWIndex',LOS_column='LOSOffset',AOS_column='AOSOffset'):\n",
    "\n",
    "    for on_item in df[conflicting_on].unique():\n",
    "        this_df = df[df[conflicting_on] == on_item ]\n",
    "        # if different_master_key:\n",
    "        #     data_dict[different_master_key]['sgk_list'] [on_item] = this_df[concat_filter].unique()\n",
    "        for csgk in this_df[concat_filter].unique():\n",
    "            that_df = this_df[this_df[concat_filter] == csgk]\n",
    "            this_LOS = list(that_df[LOS_column].unique())[0]\n",
    "            this_AOS = list(that_df[AOS_column].unique())[0]\n",
    "\n",
    "\n",
    "            that_df1 = this_df[this_df[AOS_column] >= different_setup_time  + this_LOS]\n",
    "            that_df2 = this_df[this_df[LOS_column] <= this_AOS - different_setup_time]\n",
    "            that_df3 = pd.concat([that_df1,that_df2])\n",
    "            \n",
    "            not_needed = list(that_df3[concat_filter].unique())\n",
    "            that_df = this_df[~this_df[concat_filter].isin(not_needed)]\n",
    "\n",
    "            that_df = that_df[that_df[concat_filter] != csgk]\n",
    "\n",
    "            # if different_master_key:\n",
    "            #     data_dict[different_master_key]['domain_of_csgk'] [csgk] = list(that_df[concat_filter].unique())\n",
    "            # else:\n",
    "            data_dict[csgk] = list(that_df[concat_filter].unique())\n",
    "                \n",
    "    return data_dict \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
