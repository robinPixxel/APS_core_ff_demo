{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# with open('1_input_data/config.json', 'r') as file:\n",
    "#         config = json.load(file)\n",
    "# image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "\n",
    "# image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "# image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "# image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "# image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "# image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "# image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "# base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "\n",
    "# image_opportunity_df['base_time'] = base_time_stamp\n",
    "# image_opportunity_df['req_date'] = image_opportunity_df[['base_time','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)#pd.to_datetime(image_opportunity_df['base_time']) + pd.to_timedelta(image_opportunity_df['OpportunityStartOffset'])\n",
    "# image_opportunity_df[['req_date','OpportunityStartTime','base_time','OpportunityStartOffset']]\n",
    "\n",
    "# image_opportunity_df['x'] = image_opportunity_df[['Priority','StripID']].apply(lambda a: a['Priority'] if a['Priority']<=0 else a['StripID'],axis=1)\n",
    "# image_opportunity_df[['Priority','StripID','x']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa']\n",
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa', '../']\n"
     ]
    }
   ],
   "source": [
    "#!pip list\n",
    "import os \n",
    "os.getcwd()\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 66, 'c': 43, 'd': 55, 'ww': 222}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_opportunity_df.columns\n",
    "result_dict = {'a':2,'b':66,'c':43}\n",
    "result_dict.update({'d':55,'ww':222})\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "# from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "# from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "# from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "# from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "# from model_3.MILP_imageCapture_v3_17112024_copy import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "# from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "# from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "# from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "# from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "# from result_interpret import interpret_result\n",
    "# from utils import *\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from APS_Python_core.model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from APS_Python_core.postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from APS_Python_core.model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v3_17112024_copy#MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024# \n",
    "from APS_Python_core.postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from APS_Python_core.model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from APS_Python_core.postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from APS_Python_core.result_interpret import interpret_result\n",
    "from APS_Python_core.utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['SG1K1G2K2_pair']['domain_of_csgk'])\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_and_Imaging'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    config['objective']['GS_Pass_time'] = False\n",
    "    config['objective']['GS_Pass_and_Imaging'] = False\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = True\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    #GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df_original = GS_pass_df.copy()\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    #image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    #image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.4))] + [0 for i in range(int(1.5*3600*0.6))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = GS_pass_df_original\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "def get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    #APS_Python_core/src/APS_Python_core/1_input_data/config.json\n",
    "    #with open('APS_Python_core/src/APS_Python_core/1_input_data/config.json', 'r') as file:\n",
    "    #with open('../1_input_data/config.json', 'r') as file:\n",
    "        #config = json.load(file)\n",
    "    original_image_opportunity_df = image_opportunity_df.copy()\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "    #config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "    #config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    # gs_pass_result_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    # img_capture_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    # APS_success_metric_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/APS_success_metric.csv\",index = None)\n",
    "    # downlink_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = original_image_opportunity_df\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        #v.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/\"+k+\".csv\",index = None)\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    print(\"len_of_readout=\",len(only_readout_result),only_readout_result)\n",
    "    if len(only_readout_result):\n",
    "        only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "        only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_gsPass_result = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout'][['SatID','start_time','end_time','gsID','base_time']]\n",
    "    only_gsPass_result['start_time'] = only_gsPass_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_gsPass_result['end_time'] = only_gsPass_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result,\\\n",
    "                  \"only_gsPass_result\":only_gsPass_result,\\\n",
    "                  \"combined_result\":img_capture_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "    #return result_dict\n",
    "    return result_dict\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_solving\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [1e+00, 1e+09]\n",
      "  Cost   [1e+00, 1e+00]\n",
      "  Bound  [1e+00, 1e+00]\n",
      "  RHS    [9e+04, 1e+09]\n",
      "Presolving model\n",
      "335 rows, 250 cols, 750 nonzeros  0s\n",
      "169 rows, 167 cols, 418 nonzeros  0s\n",
      "77 rows, 75 cols, 188 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   77 rows\n",
      "   75 cols (37 binary, 0 integer, 0 implied int., 38 continuous)\n",
      "   188 nonzeros\n",
      "MIP-Timing:     0.00063 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -inf            inf                  inf        0      0      0         0     0.0s\n",
      " R       0       0         0   0.00%   -3001279.839525 -34044          8715.88%        0      0      0        76     0.0s\n",
      " C       0       0         0   0.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "         1       0         1 100.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -45431\n",
      "  Dual bound        -45431\n",
      "  Gap               0% (tolerance: 0.01%)\n",
      "  P-D integral      0.0355274296334\n",
      "  Solution status   feasible\n",
      "                    -45431 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    0 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            0.00 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 0\n",
      "  Nodes             1\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     152 (total)\n",
      "                    0 (strong br.)\n",
      "                    76 (separation)\n",
      "                    0 (heuristics)\n",
      "status= Optimal\n",
      "image_capture_plan_starting\n",
      "=====IMAGING THEMAL======\n",
      "=====READOUT THERMAL======\n",
      "len_before_eclipse_transition_divide= 437\n",
      "len_after(after explode)_eclipse_transition_divide= 519\n",
      "Before_Ambiguous_event_transition_divide = 519\n",
      "Empty DataFrame\n",
      "Columns: [SatID, encoded_stripId, start_time, end_time, Eclipse, TW_index, gsID, till_now_max, prev_max, global_TW, Memory_global_TW_index, concat_sat_MGWI, EcStEnd_list, len_EcStEnd_list, new_eclipse, new_start_time, new_end_time]\n",
      "Index: []\n",
      "After_len_of_ambiguous_event_transition_divide = 519\n",
      "list of unique lenths of eclipse_transition_divide(bifurcation) =  [1 2 3]\n",
      "len_power_based_memory_based= 373 len_power_based_memory_based= 146\n",
      "final_len_power_based= 519\n",
      "GS_Pass_time_objective\n",
      "only gs check  FF01 1.0 [1.0]\n",
      "only gs check  FF02 1.0 [1.0]\n",
      "only gs check  FF03 1.0 [1.0]\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [8e-01, 2e+06]\n",
      "  Cost   [1e+00, 7e+02]\n",
      "  Bound  [1e+00, 7e+08]\n",
      "  RHS    [1e+00, 2e+08]\n",
      "Presolving model\n",
      "43052 rows, 6675 cols, 150529 nonzeros  0s\n",
      "32061 rows, 6033 cols, 103791 nonzeros  0s\n",
      "32059 rows, 6027 cols, 100925 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   32059 rows\n",
      "   6027 cols (4751 binary, 0 integer, 2 implied int., 1274 continuous)\n",
      "   100925 nonzeros\n",
      "MIP-Timing:       0.088 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -141310.402767  inf                  inf        0      0      0         0     0.1s\n",
      " R       0       0         0   0.00%   -91644.261909   -45396.632564    101.87%        0      0      0      4550     0.2s\n",
      " C       0       0         0   0.00%   -91644.261909   -45805.106186    100.07%      171     15      0      4580     0.9s\n",
      " L       0       0         0   0.00%   -91644.261909   -47547.756254     92.74%      219     36      0      4621     2.3s\n",
      " L       0       0         0   0.00%   -91644.261909   -47956.938669     91.10%      219     36      0      5456     2.7s\n",
      " L     252     241         1   0.00%   -91644.261909   -48283.582431     89.80%      226     39      7      7599     4.8s\n",
      "       848     834         3   0.00%   -91362.090848   -48283.582431     89.22%      246     42     71     13087    11.9s\n",
      "       999     965        13   0.00%   -91078.779255   -48283.582431     88.63%      325     48    250     20741    18.3s\n",
      "      1004     963        15   0.00%   -91078.779255   -48283.582431     88.63%      365     54    400     28315    25.1s\n",
      "      1018     959        23   0.00%   -91078.779255   -48283.582431     88.63%      399     60    603     35986    31.6s\n",
      "      1156     998        65   0.00%   -90800.655561   -48283.582431     88.06%      510     78   1273     45153    39.4s\n",
      "      1298    1016       117   0.00%   -90791.464154   -48283.582431     88.04%      595     99   2051     55769    46.8s\n",
      "      1434    1028       170   0.00%   -90524.54557    -48283.582431     87.49%      658    123   3054     66855    53.4s\n",
      "      1558    1048       205   0.00%   -90524.090851   -48283.582431     87.48%      677    135   4322     76454    61.8s\n",
      "      1712    1058       273   0.00%   -90515.247685   -48283.582431     87.47%      748    153   5539     86753    69.9s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 27846 rows, 3221 cols (1949 bin., 0 int., 2 impl., 1270 cont.), and 89703 nonzeros\n",
      "\n",
      "      1809       0         0   0.00%   -50284.530154   -48283.582431      4.14%      171      0      0     91956    73.7s\n",
      "      1809       0         0   0.00%   -50284.530154   -48283.582431      4.14%      171     79      4     92163    73.7s\n",
      "      2476       3       276  95.12%   -49120.206216   -48283.582431      1.73%     1258    398   1428    102590    79.0s\n",
      "      2873       0       472 100.00%   -48283.582431   -48283.582431      0.00%     1556    328   2367    106249    82.3s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -48283.5824309\n",
      "  Dual bound        -48283.5824309\n",
      "  Gap               0%\n",
      "  P-D integral      65.4419456467\n",
      "  Solution status   feasible\n",
      "                    -48283.5824309 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    5.95079541199e-14 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            82.35 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 1\n",
      "  Nodes             2873\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     106249 (total)\n",
      "                    0 (strong br.)\n",
      "                    1751 (separation)\n",
      "                    60061 (heuristics)\n",
      "status= Optimal\n",
      "total_readout_memory_objective\n",
      "only gs check  FF01 1.0 [1.0]\n",
      "only gs check  FF02 1.0 [1.0]\n",
      "only gs check  FF03 1.0 [1.0]\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [8e-01, 2e+06]\n",
      "  Cost   [6e-02, 1e+00]\n",
      "  Bound  [1e+00, 7e+08]\n",
      "  RHS    [1e+00, 2e+08]\n",
      "Presolving model\n",
      "43053 rows, 6675 cols, 150835 nonzeros  0s\n",
      "32062 rows, 6033 cols, 104097 nonzeros  0s\n",
      "32060 rows, 6027 cols, 101231 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   32060 rows\n",
      "   6027 cols (4751 binary, 0 integer, 2 implied int., 1274 continuous)\n",
      "   101231 nonzeros\n",
      "MIP-Timing:         0.1 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -10915.740299   inf                  inf        0      0      0         0     0.1s\n",
      "         0       0         0   0.00%   -8950.249415    inf                  inf        0      0      5      4085     0.2s\n",
      " L       0       0         0   0.00%   -8950.249415    -4824.913011      85.50%      233     55    317      4212     4.1s\n",
      "       357     354         2   0.00%   -8950.249415    -4824.913011      85.50%      209     51    319     15835    10.3s\n",
      "       936     927         5   0.00%   -8950.249415    -4824.913011      85.50%      242     55    344     20095    16.4s\n",
      "      1372    1358         7   0.00%   -8950.249415    -4824.913011      85.50%      255     60    375     25219    22.9s\n",
      "      1783    1765         9   0.00%   -8950.249415    -4824.913011      85.50%      275     65    400     33615    30.6s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 27838 rows, 3221 cols (1949 bin., 0 int., 0 impl., 1272 cont.), and 89985 nonzeros\n",
      "\n",
      "      2024       0         0   0.00%   -8950.249415    -4824.913011      85.50%       62      0      0     38164    35.2s\n",
      "      2024       0         0   0.00%   -8950.249415    -4824.913011      85.50%       62     46      2     38243    35.3s\n",
      " T    2542     119       150  51.24%   -8950.249391    -4962.618381      80.35%     1077    184    426     51656    39.9s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 19402 rows, 2631 cols (1362 bin., 0 int., 0 impl., 1269 cont.), and 62012 nonzeros\n",
      "\n",
      "      2946       0         0   0.00%   -8950.249391    -4962.618381      80.35%      114      0      0     54960    42.3s\n",
      "      2946       0         0   0.00%   -8925.528337    -4962.618381      79.86%      114     86     16     55778    42.3s\n",
      " T    3215      37        93  87.74%   -8614.394857    -4967.070249      73.43%      574    149    313     62436    44.1s\n",
      " T    3284      56       113  93.99%   -8614.394857    -4994.30963       72.48%      684    110    334     63643    44.6s\n",
      "      4464     193       612  98.66%   -8578.394857    -4994.30963       71.76%     1404    110   1629     72047    49.6s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 10938 rows, 1812 cols (796 bin., 0 int., 0 impl., 1016 cont.), and 36337 nonzeros\n",
      "\n",
      "      5017       0         0   0.00%   -8578.394857    -4994.30963       71.76%       88      0      0     75928    53.2s\n",
      "      5017       0         0   0.00%   -8578.394857    -4994.30963       71.76%       88     42      2     76352    53.2s\n",
      "      6369     249       541   0.00%   -8481.155493    -4994.30963       69.82%     1520    322    551     87787    58.2s\n",
      " L    6728     309       694   0.00%   -8481.155493    -5034.752667      68.45%     2032    349    704     99107    64.1s\n",
      "      7507     427      1012   0.00%   -8481.155493    -5034.752667      68.45%     2197    246   1022    118986    69.1s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 6254 rows, 1541 cols (543 bin., 0 int., 0 impl., 998 cont.), and 21641 nonzeros\n",
      "\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "      7934       0         0   0.00%   -8481.155493    -5034.752667      68.45%       78      0      0    130981    74.0s\n",
      "      7934       0         0   0.00%   -8481.155493    -5034.752667      68.45%       78     30      3    131151    74.0s\n",
      "      9329     217       582   0.20%   -8275.846171    -5034.752667      64.37%     2370    148    729    143665    79.0s\n",
      " L   10799     409      1197   0.42%   -8275.043267    -5052.551947      63.78%     2618     93   1499    158559    84.4s\n",
      "     11450     501      1474   0.83%   -8275.043267    -5052.551947      63.78%     2138     91   1851    181288    89.4s\n",
      "     13405     677      2342   2.24%   -8267.800753    -5052.551947      63.64%     3253    368   3275    205159    94.5s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 6227 rows, 1532 cols (540 bin., 0 int., 0 impl., 992 cont.), and 21556 nonzeros\n",
      "\n",
      "     14535       0         0   0.00%   -8258.149393    -5052.551947      63.45%      144      0      0    218452    97.4s\n",
      "     14535       0         0   0.00%   -8258.149393    -5052.551947      63.45%      144     56      3    218663    97.4s\n",
      " T   16513     207       860  23.80%   -8212.714172    -5178.456857      58.59%     2936     97   1985    232244   101.4s\n",
      " T   16521     203       864  23.80%   -8212.714172    -5196.181857      58.05%     2937     97   1997    232257   101.4s\n",
      " T   16601     215       893  23.80%   -8212.714172    -5203.139355      57.84%     2873    111   2110    232655   101.5s\n",
      " T   16608     197       896  23.80%   -8212.714172    -5219.964355      57.33%     2874    111   2114    232695   101.5s\n",
      " T   16616     183       899  23.80%   -8212.714172    -5225.937688      57.15%     2876    111   2119    232745   101.6s\n",
      " T   16622     146       901  23.81%   -8212.714172    -5245.009831      56.58%     2877    111   2121    232774   101.6s\n",
      " T   16626     147       903  23.81%   -8212.714172    -5246.689831      56.53%     2878    111   2125    232779   101.6s\n",
      " T   16634     145       906  23.81%   -8212.714172    -5261.834831      56.08%     2880    111   2129    232813   101.6s\n",
      " T   16641     144       909  23.81%   -8212.714172    -5263.514831      56.03%     2882    111   2134    232843   101.6s\n",
      " T   16656     145       915  23.81%   -8212.714172    -5265.062963      55.99%     2887    111   2142    232927   101.6s\n",
      " T   17979     278      1484  37.74%   -8110.588009    -5267.16564       53.98%     2890    223   3598    248058   105.0s\n",
      " T   17992     272      1488  37.74%   -8110.588009    -5268.84564       53.93%     2892    223   3605    248102   105.0s\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      " T   18020     266      1497  37.74%   -8110.588009    -5271.620186      53.85%     2896    223   3623    248228   105.0s\n",
      " T   18035     267      1502  37.74%   -8110.588009    -5273.168318      53.81%     2900    223   3629    248330   105.1s\n",
      " T   18104     274      1528  37.74%   -8110.588009    -5273.254212      53.81%     2912    261   3691    249833   105.3s\n",
      " T   18137     271      1542  37.74%   -8110.588009    -5274.934212      53.76%     2914    261   3727    249942   105.4s\n",
      " T   19177     236      1982  38.12%   -8110.588009    -5567.906268      45.67%     3919    406   4966    257631   107.3s\n",
      " T   19190     223      1989  38.12%   -8110.588009    -5585.181268      45.22%     3919    406   4986    257651   107.3s\n",
      " T   19202     223      1995  38.12%   -8110.588009    -5586.175554      45.19%     3920    406   5002    257690   107.3s\n",
      " T   19266     223      2023  38.14%   -8030.675146    -5605.06456       43.28%     3977    425   5086    258292   107.5s\n",
      " T   19276     209      2028  38.14%   -8030.675146    -5620.98956       42.87%     3978    425   5100    258303   107.5s\n",
      " T   19287     209      2032  38.14%   -8030.675146    -5621.983845      42.84%     3980    425   5108    258343   107.5s\n",
      " T   19302     209      2038  38.14%   -8030.675146    -5623.792237      42.80%     3982    425   5122    258422   107.5s\n",
      " T   19310     209      2041  38.14%   -8030.675146    -5624.786523      42.77%     3984    425   5126    258465   107.5s\n",
      "     20914     348      2747  39.56%   -7678.181584    -5624.786523      36.51%     5078    226   7233    294528   112.5s\n",
      " T   21290     362      2918  50.37%   -7572.697704    -5625.092677      34.62%     5101    168   7676    304565   114.0s\n",
      " T   22171     406      3309  52.01%   -7401.202474    -5628.235094      31.50%     5395    294   8624    323922   116.7s\n",
      "     23565     463      3939  58.35%   -7259.096452    -5628.235094      28.98%     5726    446   7603    365626   121.9s\n",
      " T   24742     218      4492  64.41%   -7251.899981    -5871.394237      23.51%     3912     58   6651    398210   126.1s\n",
      " T   24830     222      4532  64.41%   -7251.899981    -5871.519237      23.51%     3890     77   6742    398560   126.2s\n",
      " T   25688     188      4942  70.03%   -7213.132498    -6085.164237      18.54%     3292     57   7160    408414   128.3s\n",
      " T   25696     185      4943  73.15%   -7213.132498    -6095.65257       18.33%     3293     57   7163    408458   128.3s\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      " T   25874     193      5021  73.15%   -7197.657008    -6170.581924      16.64%     3369    221   7367    410360   128.7s\n",
      " T   25931     185      5047  73.15%   -7197.657008    -6171.753353      16.62%     3073     61   7348    410568   128.8s\n",
      " T   26649     152      5402  82.93%   -7133.357524    -6201.234155      15.03%     6763    207   7048    446715   132.7s\n",
      "     27281     128      5720  97.28%   -7093.258018    -6201.234155      14.38%     7721    231   5993    503001   137.8s\n",
      "     27842      98      6007  98.06%   -7027.989455    -6201.234155      13.33%     5687    200   4665    558585   142.8s\n",
      "     28561      90      6365  98.87%   -7001.626764    -6201.234155      12.91%     6647    332   3391    607414   147.8s\n",
      "     29381      52      6786  99.17%   -6936.151773    -6201.234155      11.85%     5639    200   3405    654603   152.9s\n",
      "     30108      38      7149  99.75%   -6851.175846    -6201.234155      10.48%     7082    331   3551    700046   158.0s\n",
      "     30536       0      7376 100.00%   -6201.234155    -6201.234155       0.00%     8763    200   3626    728918   161.1s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -6201.23415506\n",
      "  Dual bound        -6201.23415506\n",
      "  Gap               0%\n",
      "  P-D integral      86.8585993346\n",
      "  Solution status   feasible\n",
      "                    -6201.23415506 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    2.44249065418e-15 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            161.08 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 6\n",
      "  Nodes             30536\n",
      "  Repair LPs        9 (9 feasible; 402 iterations)\n",
      "  LP iterations     728918 (total)\n",
      "                    39947 (strong br.)\n",
      "                    78431 (separation)\n",
      "                    70344 (heuristics)\n",
      "status= Optimal\n",
      "Downlink_plan_starting\n",
      "len_of_readout= 27    SatID  start_time  end_time           base_time\n",
      "0   FF01     91429.0   91496.5 2024-11-13 12:30:44\n",
      "1   FF02     91539.0   91598.0 2024-11-13 12:30:44\n",
      "2   FF01     95331.0   95391.0 2024-11-13 12:30:44\n",
      "3   FF02     95975.0   96034.0 2024-11-13 12:30:44\n",
      "4   FF01     99811.0   99871.0 2024-11-13 12:30:44\n",
      "5   FF02    102383.0  102442.0 2024-11-13 12:30:44\n",
      "6   FF01    105579.0  105639.0 2024-11-13 12:30:44\n",
      "7   FF02    107975.0  108034.0 2024-11-13 12:30:44\n",
      "8   FF01    111801.0  111861.0 2024-11-13 12:30:44\n",
      "9   FF02    113553.0  113612.0 2024-11-13 12:30:44\n",
      "10  FF01    119089.0  119149.0 2024-11-13 12:30:44\n",
      "11  FF02    119089.0  119148.0 2024-11-13 12:30:44\n",
      "12  FF02    121325.0  121384.0 2024-11-13 12:30:44\n",
      "13  FF01    122957.0  123017.0 2024-11-13 12:30:44\n",
      "14  FF02    126832.0  126891.0 2024-11-13 12:30:44\n",
      "15  FF01    128767.0  128827.0 2024-11-13 12:30:44\n",
      "16  FF02    132125.0  132184.0 2024-11-13 12:30:44\n",
      "17  FF01    134587.0  134647.0 2024-11-13 12:30:44\n",
      "18  FF02    137703.0  137759.5 2024-11-13 12:30:44\n",
      "19  FF01    140451.0  140511.0 2024-11-13 12:30:44\n",
      "20  FF01    142925.0  142985.0 2024-11-13 12:30:44\n",
      "21  FF02    142925.0  142975.0 2024-11-13 12:30:44\n",
      "22  FF01    148325.0  148385.0 2024-11-13 12:30:44\n",
      "23  FF02    150278.0  150328.0 2024-11-13 12:30:44\n",
      "24  FF01    154086.0  154146.0 2024-11-13 12:30:44\n",
      "25  FF02    155885.0  155935.0 2024-11-13 12:30:44\n",
      "26  FF02    160675.0  160725.0 2024-11-13 12:30:44\n"
     ]
    }
   ],
   "source": [
    "GS_pass_df = GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "image_opportunity_df = pd.read_csv(\"1_input_data/Imaging_new (1) copy.csv\")\n",
    "image_downlink_df = pd.read_csv(\"1_input_data/APS_imageTable_TV1.csv\")\n",
    "with open('1_input_data/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "result_dict = get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n",
    "for k,v in result_dict.items():\n",
    "    v.to_csv('5_output_data/'+k+'.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-48283.5824309=25 strips 83 gspasses,-48283.5824309=25 strips 83 gspasses,-48283.5824309=26 strips 83 gspasses ,  -48283.5824309= 24 strips 82 gspasses , -48283.5824309 25 strips 82 gs PASss,-48283.5824309 24 strips 83 gs PASss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GS_pass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(image_opportunity_df['Priority'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['only_readout_result', 'only_img_capture_result', 'only_gsPass_result', 'combined_result', 'interpret_extracted_raw_file_df', 'interpret_selected_oppr_conflict_comparision_df', 'interpret_KPI_df'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 83)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['only_img_capture_result']['StripID'].nunique(),len(result_dict['only_gsPass_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SatID</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>gsID</th>\n",
       "      <th>base_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>FF01</td>\n",
       "      <td>2024-11-14 12:36:49</td>\n",
       "      <td>2024-11-14 12:44:54.000000</td>\n",
       "      <td>JEJU01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>FF01</td>\n",
       "      <td>2024-11-14 12:51:35</td>\n",
       "      <td>2024-11-14 13:02:18.000000</td>\n",
       "      <td>SVALBARD01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>FF02</td>\n",
       "      <td>2024-11-14 13:07:41</td>\n",
       "      <td>2024-11-14 13:17:44.000000</td>\n",
       "      <td>JEJU01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>FF02</td>\n",
       "      <td>2024-11-14 13:23:42</td>\n",
       "      <td>2024-11-14 13:34:21.000000</td>\n",
       "      <td>SVALBARD01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>FF03</td>\n",
       "      <td>2024-11-14 13:39:21</td>\n",
       "      <td>2024-11-14 13:50:02.000000</td>\n",
       "      <td>JEJU01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>FF03</td>\n",
       "      <td>2024-11-15 11:58:56</td>\n",
       "      <td>2024-11-15 12:09:42.000000</td>\n",
       "      <td>AWARUA01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>FF02</td>\n",
       "      <td>2024-11-15 12:06:06</td>\n",
       "      <td>2024-11-15 12:09:42.000000</td>\n",
       "      <td>SVALBARD01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>FF03</td>\n",
       "      <td>2024-11-15 12:24:20</td>\n",
       "      <td>2024-11-15 12:30:59.000000</td>\n",
       "      <td>JEJU01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>FF03</td>\n",
       "      <td>2024-11-15 12:38:14</td>\n",
       "      <td>2024-11-15 12:48:58.000000</td>\n",
       "      <td>SVALBARD01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>FF03</td>\n",
       "      <td>2024-11-15 13:36:50</td>\n",
       "      <td>2024-11-15 13:42:28.173489</td>\n",
       "      <td>AWARUA01</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SatID          start_time                   end_time        gsID  \\\n",
       "53   FF01 2024-11-14 12:36:49 2024-11-14 12:44:54.000000      JEJU01   \n",
       "54   FF01 2024-11-14 12:51:35 2024-11-14 13:02:18.000000  SVALBARD01   \n",
       "55   FF02 2024-11-14 13:07:41 2024-11-14 13:17:44.000000      JEJU01   \n",
       "56   FF02 2024-11-14 13:23:42 2024-11-14 13:34:21.000000  SVALBARD01   \n",
       "57   FF03 2024-11-14 13:39:21 2024-11-14 13:50:02.000000      JEJU01   \n",
       "..    ...                 ...                        ...         ...   \n",
       "131  FF03 2024-11-15 11:58:56 2024-11-15 12:09:42.000000    AWARUA01   \n",
       "132  FF02 2024-11-15 12:06:06 2024-11-15 12:09:42.000000  SVALBARD01   \n",
       "133  FF03 2024-11-15 12:24:20 2024-11-15 12:30:59.000000      JEJU01   \n",
       "134  FF03 2024-11-15 12:38:14 2024-11-15 12:48:58.000000  SVALBARD01   \n",
       "135  FF03 2024-11-15 13:36:50 2024-11-15 13:42:28.173489    AWARUA01   \n",
       "\n",
       "              base_time  \n",
       "53  2024-11-13 12:30:44  \n",
       "54  2024-11-13 12:30:44  \n",
       "55  2024-11-13 12:30:44  \n",
       "56  2024-11-13 12:30:44  \n",
       "57  2024-11-13 12:30:44  \n",
       "..                  ...  \n",
       "131 2024-11-13 12:30:44  \n",
       "132 2024-11-13 12:30:44  \n",
       "133 2024-11-13 12:30:44  \n",
       "134 2024-11-13 12:30:44  \n",
       "135 2024-11-13 12:30:44  \n",
       "\n",
       "[83 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['only_gsPass_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#capture_plan_data_input['']\n",
    "#-48283.5824309\n",
    "#-48271\n",
    "len(result_dict['interpret_selected_oppr_conflict_comparision_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conflic_strip_flag_named</th>\n",
       "      <th>max_Norm_TP</th>\n",
       "      <th>this_flag_norm_TP</th>\n",
       "      <th>max_Norm_GP</th>\n",
       "      <th>this_flag_norm_GP</th>\n",
       "      <th>max_Norm_LPDD</th>\n",
       "      <th>this_flag_norm_LLDD</th>\n",
       "      <th>base_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FF01_Order 11 - Strip 1_Area 13_1.0</td>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FF01_Order 22 - Strip 3_Area 21_1.0</td>\n",
       "      <td>568</td>\n",
       "      <td>568</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FF01_Order 30 - Strip 0_Area 27_1.0</td>\n",
       "      <td>555</td>\n",
       "      <td>555</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FF01_Order 47 - Strip 5_Area 35_1.0</td>\n",
       "      <td>506</td>\n",
       "      <td>506</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FF01_Order 57 - Strip 0_Area 36_1.0</td>\n",
       "      <td>725</td>\n",
       "      <td>725</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>FF01_Order 64 - Strip 3_Area 37_1.0</td>\n",
       "      <td>555</td>\n",
       "      <td>555</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>FF01_Order 67 - Strip 0_Area 6_1.0</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>FF01_Order 71 - Strip 1_Area 8_1.0</td>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>FF02_Order 4 - Strip 3_Area 0_1.0</td>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>FF02_Order 14 - Strip 3_Area 18_1.0</td>\n",
       "      <td>554</td>\n",
       "      <td>554</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>FF02_Order 28 - Strip 3_Area 24_1.0</td>\n",
       "      <td>401</td>\n",
       "      <td>401</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>FF02_Order 29 - Strip 0_Area 26_1.0</td>\n",
       "      <td>409</td>\n",
       "      <td>409</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FF02_Order 44 - Strip 3_Area 35_1.0</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>FF02_Order 46 - Strip 3_Area 35_1.0</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>FF02_Order 53 - Strip 3_Area 36_1.0</td>\n",
       "      <td>591</td>\n",
       "      <td>591</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>FF03_Order 0 - Strip 4_Area 0_1.0</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>FF03_Order 9 - Strip 3_Area 13_1.0</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>FF03_Order 18 - Strip 1_Area 18_1.0</td>\n",
       "      <td>612</td>\n",
       "      <td>612</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>FF03_Order 20 - Strip 3_Area 20_1.0</td>\n",
       "      <td>552</td>\n",
       "      <td>552</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>FF03_Order 23 - Strip 1_Area 23_1.0</td>\n",
       "      <td>491</td>\n",
       "      <td>491</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>FF03_Order 41 - Strip 1_Area 34_1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>FF03_Order 45 - Strip 0_Area 35_1.0</td>\n",
       "      <td>485</td>\n",
       "      <td>485</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>FF03_Order 48 - Strip 1_Area 35_1.0</td>\n",
       "      <td>498</td>\n",
       "      <td>498</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>908.001084</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>FF03_Order 59 - Strip 3_Area 36_1.0</td>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>FF03_Order 65 - Strip 2_Area 37_1.0</td>\n",
       "      <td>587</td>\n",
       "      <td>587</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>752.280794</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>FF03_Order 72 - Strip 1_Area 8_1.0</td>\n",
       "      <td>403</td>\n",
       "      <td>402</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2024-11-13 12:30:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                conflic_strip_flag_named  max_Norm_TP  this_flag_norm_TP  \\\n",
       "0    FF01_Order 11 - Strip 1_Area 13_1.0          403                403   \n",
       "6    FF01_Order 22 - Strip 3_Area 21_1.0          568                568   \n",
       "11   FF01_Order 30 - Strip 0_Area 27_1.0          555                555   \n",
       "16   FF01_Order 47 - Strip 5_Area 35_1.0          506                506   \n",
       "28   FF01_Order 57 - Strip 0_Area 36_1.0          725                725   \n",
       "49   FF01_Order 64 - Strip 3_Area 37_1.0          555                555   \n",
       "56    FF01_Order 67 - Strip 0_Area 6_1.0          700                700   \n",
       "72    FF01_Order 71 - Strip 1_Area 8_1.0          420                420   \n",
       "80     FF02_Order 4 - Strip 3_Area 0_1.0          525                525   \n",
       "91   FF02_Order 14 - Strip 3_Area 18_1.0          554                554   \n",
       "95   FF02_Order 28 - Strip 3_Area 24_1.0          401                401   \n",
       "96   FF02_Order 29 - Strip 0_Area 26_1.0          409                409   \n",
       "98   FF02_Order 44 - Strip 3_Area 35_1.0          488                488   \n",
       "105  FF02_Order 46 - Strip 3_Area 35_1.0          542                542   \n",
       "118  FF02_Order 53 - Strip 3_Area 36_1.0          591                591   \n",
       "131    FF03_Order 0 - Strip 4_Area 0_1.0          504                504   \n",
       "151   FF03_Order 9 - Strip 3_Area 13_1.0          404                404   \n",
       "156  FF03_Order 18 - Strip 1_Area 18_1.0          612                612   \n",
       "170  FF03_Order 20 - Strip 3_Area 20_1.0          552                552   \n",
       "171  FF03_Order 23 - Strip 1_Area 23_1.0          491                491   \n",
       "181  FF03_Order 41 - Strip 1_Area 34_1.0          701                701   \n",
       "196  FF03_Order 45 - Strip 0_Area 35_1.0          485                485   \n",
       "200  FF03_Order 48 - Strip 1_Area 35_1.0          498                498   \n",
       "209  FF03_Order 59 - Strip 3_Area 36_1.0          570                570   \n",
       "226  FF03_Order 65 - Strip 2_Area 37_1.0          587                587   \n",
       "233   FF03_Order 72 - Strip 1_Area 8_1.0          403                402   \n",
       "\n",
       "     max_Norm_GP  this_flag_norm_GP  max_Norm_LPDD  this_flag_norm_LLDD  \\\n",
       "0          500.0              500.0    1000.000000          1000.000000   \n",
       "6         1000.0             1000.0     752.280794           752.280794   \n",
       "11        1000.0             1000.0     752.280794           752.280794   \n",
       "16         750.0              750.0     908.001084           908.001084   \n",
       "28        1000.0             1000.0     752.280794           752.280794   \n",
       "49        1000.0             1000.0     752.280794           752.280794   \n",
       "56        1000.0             1000.0     908.001084           752.280794   \n",
       "72         500.0              500.0    1000.000000          1000.000000   \n",
       "80         750.0              750.0     908.001084           908.001084   \n",
       "91        1000.0             1000.0     752.280794           752.280794   \n",
       "95         500.0              500.0    1000.000000          1000.000000   \n",
       "96         500.0              500.0    1000.000000          1000.000000   \n",
       "98         750.0              750.0     908.001084           908.001084   \n",
       "105        750.0              750.0     908.001084           908.001084   \n",
       "118       1000.0             1000.0     752.280794           752.280794   \n",
       "131        750.0              750.0     908.001084           908.001084   \n",
       "151        500.0              500.0    1000.000000          1000.000000   \n",
       "156       1000.0             1000.0     752.280794           752.280794   \n",
       "170       1000.0             1000.0     752.280794           752.280794   \n",
       "171        750.0              750.0     908.001084           908.001084   \n",
       "181        500.0              500.0    1000.000000          1000.000000   \n",
       "196        750.0              750.0     908.001084           908.001084   \n",
       "200        750.0              750.0     908.001084           908.001084   \n",
       "209       1000.0             1000.0     752.280794           752.280794   \n",
       "226       1000.0             1000.0     752.280794           752.280794   \n",
       "233        500.0              500.0    1000.000000          1000.000000   \n",
       "\n",
       "              base_time  \n",
       "0   2024-11-13 12:30:44  \n",
       "6   2024-11-13 12:30:44  \n",
       "11  2024-11-13 12:30:44  \n",
       "16  2024-11-13 12:30:44  \n",
       "28  2024-11-13 12:30:44  \n",
       "49  2024-11-13 12:30:44  \n",
       "56  2024-11-13 12:30:44  \n",
       "72  2024-11-13 12:30:44  \n",
       "80  2024-11-13 12:30:44  \n",
       "91  2024-11-13 12:30:44  \n",
       "95  2024-11-13 12:30:44  \n",
       "96  2024-11-13 12:30:44  \n",
       "98  2024-11-13 12:30:44  \n",
       "105 2024-11-13 12:30:44  \n",
       "118 2024-11-13 12:30:44  \n",
       "131 2024-11-13 12:30:44  \n",
       "151 2024-11-13 12:30:44  \n",
       "156 2024-11-13 12:30:44  \n",
       "170 2024-11-13 12:30:44  \n",
       "171 2024-11-13 12:30:44  \n",
       "181 2024-11-13 12:30:44  \n",
       "196 2024-11-13 12:30:44  \n",
       "200 2024-11-13 12:30:44  \n",
       "209 2024-11-13 12:30:44  \n",
       "226 2024-11-13 12:30:44  \n",
       "233 2024-11-13 12:30:44  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['interpret_selected_oppr_conflict_comparision_df']#['conflic_strip_flag_named'].nunique()##['interpret_selected_oppr_conflict_comparision_df']#['interpret_extracted_raw_file_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlink_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'extracted_raw_file_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m l1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_raw_file_df\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_oppr_conflict_comparision_df\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKPI_df\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df111 \u001b[38;5;241m=\u001b[39m result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_raw_file_df\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'extracted_raw_file_df'"
     ]
    }
   ],
   "source": [
    "l1 = 'extracted_raw_file_df', 'selected_oppr_conflict_comparision_df', 'KPI_df'\n",
    "df111 = result_dict['extracted_raw_file_df']#.columns\n",
    "#interpret_result_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = img_capture_result[img_capture_result['operation']=='Imaging']#['StripID']#.nunique()\n",
    "Z = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout']\n",
    "Z['duration'] = Z['end_time']- Z['start_time']\n",
    "#Y[Y['StripID']=='']\n",
    "\n",
    "#Y[Y['encoded_strip_id']=='Order 1 - Strip 0_Area 0']\n",
    "Y['StripID'].nunique(),len(Y),len(Z),Z['duration'].sum(),gs_pass_result_df['duration'].sum(),len(gs_pass_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['SatID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = Y[Y['SatID']=='FF03']\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "CR = RZO[RZO['SatID']=='FF03']\n",
    "pd.concat([CI,CR]).sort_values(by='start_time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Order 1 - Strip 0_Area 0'\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "RZO[RZO['SatID']=='FF01']\n",
    "RZO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RZ = capture_plan_data_input['dedicated_readout_df']\n",
    "len(RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF02'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF03'].sort_values(by='start_time'))\n",
    "RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_capture_result['operation'].unique()\n",
    "\n",
    "[ item for item in capture_plan_data_input['Memory_NoimageGs_TW_list'] if item[2]=='FF01']\n",
    "#capture_plan_data_input['dedicatedReadoutTWlist__concat_sat_memoryTWindex']['FF01_136.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_eclipse_data.csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eclipse_df[eclipse_df['eclipse']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_df.sort_values(by='time_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SatID,start_time,end_time,eclipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['GS_pass_df', 'image_opportunity_df', 'image_downlink_df', 'eclipse_df_dict', 'config']\n",
    "#input_dict['eclipse_df_dict']['FF02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['readout_memory_capacity__s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['imgery_sat_id_list']#.keys()\n",
    "[s+'_'+str(n) for s in capture_plan_data_input['imgery_sat_id_list']+capture_plan_data_input['only_gs_sat_id_list'] \\\n",
    " if s in capture_plan_data_input['dedicatedReadoutTWIndex__sat'].keys() for n in capture_plan_data_input['dedicatedReadoutTWIndex__sat'][s]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "\n",
    "#eclipse_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['assured_tasking_based_on_input_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['cs1j2k2Domainlist__cs1j1k1']['FF01_Order 1 - Strip 0_Area 0_1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['GS_Pass_time_objective'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(img_capture_result[img_capture_result['operation']=='Imaging'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "memory_plot_df = result_dict['combined_result']\n",
    "\n",
    "#memory_plot_df.columns\n",
    "#memory_plot_df[['SatID','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','base_time','OpportunityStartOffset','OpportunityEndOffset#']]\n",
    "memory_plot_df = memory_plot_df[['SatID','start_time','end_time','operation','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','base_time']]\n",
    "memory_plot_df.sort_values(by=['SatID','start_time'],inplace=True)\n",
    "\n",
    "memory_plot_df['till_now_max'] = memory_plot_df.groupby('SatID')['end_time'].cummax()\n",
    "memory_plot_df['prev_max'] = memory_plot_df.groupby('SatID')['till_now_max'].shift(1)\n",
    "\n",
    "memory_plot_df1 = memory_plot_df[memory_plot_df['start_time'] > memory_plot_df['prev_max'] + 1] \n",
    "memory_plot_df1['start_time1'] = memory_plot_df1['prev_max'] + 1 #TODO1 +1 is okay ?\n",
    "memory_plot_df1['end_time1'] = memory_plot_df1['start_time'] - 1\n",
    "memory_plot_df1['operation'] = 'idle'\n",
    "\n",
    "memory_plot_df1 = memory_plot_df1[['SatID','start_time1','end_time1','operation','base_time']]\n",
    "\n",
    "#memory_plot_df1 = memory_plot_df1.drop(['start_time', 'end_time','till_now_max','prev_max'], axis=1)\n",
    "memory_plot_df1.rename(columns={'start_time1':'start_time','end_time1':'end_time'},inplace=True)\n",
    "#imgGS_union_df1 ==> contains TW without img and without gs pass  table without eclipse divide\n",
    "final_memory_plot_df = pd.concat([memory_plot_df,memory_plot_df1])\n",
    "final_memory_plot_df.sort_values(by=['SatID','start_time'],inplace=True)\n",
    "\n",
    "final_memory_plot_df['camera_memory_value_endofTW'] = final_memory_plot_df['camera_memory_value_endofTW'].ffill()\n",
    "final_memory_plot_df['delta_camera_memory_value_in_this_TW'] = final_memory_plot_df['delta_camera_memory_value_in_this_TW'].fillna(0)\n",
    "final_memory_plot_df['start_time'] = final_memory_plot_df[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "final_memory_plot_df['end_time'] = final_memory_plot_df[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "final_memory_plot_df = final_memory_plot_df[['SatID','start_time','end_time','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','operation','base_time']]\n",
    "sat_list = final_memory_plot_df['SatID'].unique()\n",
    "#sat_list = ['FF01']\n",
    "\n",
    "\n",
    "colors = {\n",
    "        \"Imaging\": \"blue\",\n",
    "        \"downlinking_from_Readout\": \"green\",\n",
    "        \"Readout\": \"red\",\n",
    "        \"idle\": \"gray\"\n",
    "    }\n",
    "\n",
    "\n",
    "for s in sat_list:\n",
    "    this_plot_df = final_memory_plot_df[final_memory_plot_df['SatID']==s]\n",
    "    start_time_list = this_plot_df['start_time'].to_list()\n",
    "    end_time_list = this_plot_df['end_time'].to_list()\n",
    "    operation_list = this_plot_df['operation'].to_list()\n",
    "\n",
    "    camera_memory_value_endofTW_list = this_plot_df['camera_memory_value_endofTW'].to_list()\n",
    "    this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_endofTW'].shift(1)\n",
    "\n",
    "    first_row_list = this_plot_df.values.tolist()[0]\n",
    "    first_row_operation = first_row_list[5]\n",
    "    first_row_memory_val = first_row_list[3]\n",
    "    ortherwise_memory_val =  first_row_list[3] - first_row_list[4]\n",
    "    if first_row_operation not in['Imaging','Readout']:\n",
    "        this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_startofTW_list'].fillna(first_row_memory_val)\n",
    "    else:\n",
    "        this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_startofTW_list'].fillna(ortherwise_memory_val)\n",
    "\n",
    "    camera_memory_value_StartofTW_list = this_plot_df['camera_memory_value_startofTW_list'].to_list()\n",
    "    \n",
    "    time_list = list(itertools.chain.from_iterable(zip(start_time_list,end_time_list )))\n",
    "    operation_list = list(itertools.chain.from_iterable(zip(operation_list,operation_list )))\n",
    "    camera_memory_value_list = list(itertools.chain.from_iterable(zip(camera_memory_value_StartofTW_list,camera_memory_value_endofTW_list )))\n",
    "    df = pd.DataFrame({'time':time_list,'operation':operation_list,'memory':camera_memory_value_list})\n",
    "\n",
    "    this_list = this_plot_df.values.tolist()\n",
    "    # Plot using Plotly Express\n",
    "    # fig = px.line(this_plot_df, x='start_time', y='camera_memory_value_endofTW', color='operation', line_group='operation',\n",
    "    #             title=\"Memory Usage of Different Operations\")\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #                         x=time_list,\n",
    "    #                         y=camera_memory_value_list,\n",
    "    #                         mode='lines+markers+text',\n",
    "    #                         #name=operation_list,\n",
    "    #                         #line=dict(color=colors.get(operation, 'black'), width=2),\n",
    "    #                         #text=[f\"({start_time}, {current_memory:.2f})\", f\"({end_time}, {end_memory:.2f})\"],\n",
    "    #                         #textposition=text_position\n",
    "    #                     ))\n",
    "    \n",
    "    # fig.show()\n",
    "    \n",
    "    for item in this_list:\n",
    "            start_time = item[1]\n",
    "            end_time = item[2]\n",
    "            current_memory = item[6]\n",
    "            end_memory = item[3]\n",
    "            operation = item[5]\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                            x=[start_time, end_time],\n",
    "                            y=[current_memory, end_memory],\n",
    "                            mode='lines+markers+text',\n",
    "                            name=operation+'_'+s,\n",
    "                            line=dict(color=colors.get(operation, 'black'), width=2),\n",
    "                            #text=[f\"({start_time}, {current_memory:.2f})\", f\"({end_time}, {end_memory:.2f})\"],\n",
    "                            #textposition=text_position\n",
    "                        ))\n",
    "\n",
    "    #     # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Memory Profile Over Time\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Memory\",\n",
    "        legend_title=\"Operations\",\n",
    "          )\n",
    "    # names = set()\n",
    "    # fig.for_each_trace(\n",
    "    #     lambda trace:\n",
    "    #         trace.update(showlegend=False)\n",
    "    #         if (trace.name in names) else names.add(trace.name))\n",
    "\n",
    "    # Save figure as HTML\n",
    "    html_filename = \"memory_profile_.html\"\n",
    "    fig.write_html('5_output_data'+'/'+html_filename)\n",
    "\n",
    "# final_memory_plot_df[final_memory_plot_df['SatID']=='FF01'].sort_values(by='start_time')\n",
    "\n",
    "import pygwalker as pyg\n",
    "pyg.walk(final_memory_plot_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import os\n",
    "from preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from result_interpret import interpret_result\n",
    "from utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_time'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    # data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 2  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = True\n",
    "    # #config['objective']['total_readout_memory'] = False\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    # data['total_priority_objective'] = obj_model.prob.objective.value()\n",
    "    #++++++++++++++++++++++++++  STEP 3  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    \n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.6))] + [0 for i in range(int(1.5*3600*0.4))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    with open('1_input_data/config.json', 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "        config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "        config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    try:\n",
    "        print(dfd)\n",
    "        downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    except:\n",
    "        print(\"downlink_schedule_has_some_error\")\n",
    "        downlink_result = pd.DataFrame()\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    gs_pass_result_df.to_csv(\"5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    img_capture_result.to_csv(\"5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    APS_success_metric_df.to_csv(\"5_output_data/APS_success_metric.csv\",index = None)\n",
    "    downlink_result.to_csv(\"5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        v.to_csv(\"5_output_data/\"+k+\".csv\",index = None)\n",
    "\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conflicting_dict(self,df,data_dict,different_setup_time,conflicting_on = 'GsID',concat_filter ='concat_gsid_satid_TWIndex',LOS_column='LOSOffset',AOS_column='AOSOffset'):\n",
    "\n",
    "    for on_item in df[conflicting_on].unique():\n",
    "        this_df = df[df[conflicting_on] == on_item ]\n",
    "        # if different_master_key:\n",
    "        #     data_dict[different_master_key]['sgk_list'] [on_item] = this_df[concat_filter].unique()\n",
    "        for csgk in this_df[concat_filter].unique():\n",
    "            that_df = this_df[this_df[concat_filter] == csgk]\n",
    "            this_LOS = list(that_df[LOS_column].unique())[0]\n",
    "            this_AOS = list(that_df[AOS_column].unique())[0]\n",
    "\n",
    "\n",
    "            that_df1 = this_df[this_df[AOS_column] >= different_setup_time  + this_LOS]\n",
    "            that_df2 = this_df[this_df[LOS_column] <= this_AOS - different_setup_time]\n",
    "            that_df3 = pd.concat([that_df1,that_df2])\n",
    "            \n",
    "            not_needed = list(that_df3[concat_filter].unique())\n",
    "            that_df = this_df[~this_df[concat_filter].isin(not_needed)]\n",
    "\n",
    "            that_df = that_df[that_df[concat_filter] != csgk]\n",
    "\n",
    "            # if different_master_key:\n",
    "            #     data_dict[different_master_key]['domain_of_csgk'] [csgk] = list(that_df[concat_filter].unique())\n",
    "            # else:\n",
    "            data_dict[csgk] = list(that_df[concat_filter].unique())\n",
    "                \n",
    "    return data_dict \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def evaluate_eqn(t,temp_eqn):\n",
    "    t = t\n",
    "    return eval(temp_eqn)\n",
    "\n",
    "\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "heat_eqn = \"0.0000000728690314 * t**3 - 0.000138692964 * t**2 + 0.103057817 * t  + 1.88504399 \"\n",
    "# y = \"math.exp(x)\"\n",
    "T = 25\n",
    "initial_temp = 25\n",
    "limit_temp = initial_temp + 3\n",
    "\n",
    "\n",
    "a20 = -13.339128\t\n",
    "b20 = 0.504271\t\n",
    "c20 = 0.581774\n",
    "\n",
    "a40 = -13.503356\t\n",
    "b40 = 0.655062\t\n",
    "c40 = 0.397629\n",
    "\n",
    "y20 = 54.9\n",
    "y40 = 70.6\n",
    "\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    " \n",
    "Y = [[-13.339128,-13.503356],[0.504271,0.655062],[0.581774,0.397629],[y20,y40]]\n",
    "X = [[20,40],[20,40],[20,40],[20,40]]\n",
    " \n",
    "# test value\n",
    "interpolate_x = initial_temp\n",
    " \n",
    "# Finding the interpolation\n",
    "l1 =[]\n",
    "for i in range(4):\n",
    "    y_interp = interp1d(X[i], Y[i])\n",
    "    l1.append(y_interp(interpolate_x))\n",
    "\n",
    "# print(\"Value of Y at x = {} is\".format(interpolate_x),\n",
    "#       y_interp(interpolate_x))\n",
    "cool_eqn = str(l1[2])+\"*math.exp(1*t*\"+str(l1[0])+\")+\"+str(l1[1])\n",
    "x = 2\n",
    "#eval(y)\n",
    "temp_profile_df = pd.DataFrame()\n",
    "for i in range(1):\n",
    "    df = pd.DataFrame({\"time_index\":[i for i in range(1,T*60)]})\n",
    "    df['epoch'] = str(i)+'_'+'heat'\n",
    "    df['delta_temp_heat'] = df['time_index'].apply(lambda a:evaluate_eqn(a,temp_eqn = heat_eqn) ) \n",
    "    df['initial_temp'] =  initial_temp\n",
    "    df['temp']=df['delta_temp_heat']  + df['initial_temp']\n",
    "    df = df[df['temp']<=70]\n",
    "    #print(df)\n",
    "    max_heat_temp = df['temp'].max()\n",
    "    print(max_heat_temp)\n",
    "    df_cool = pd.DataFrame({\"time_index\":[i for i in range(1,T*60)]})\n",
    "    df_cool['epoch'] = str(i)+'_'+'cool'\n",
    "    df_cool[\"time_index_new\"] = df_cool[\"time_index\"]/1800\n",
    "    df_cool['delta_temp'] = df_cool['time_index_new'].apply(lambda a:evaluate_eqn(a,temp_eqn = cool_eqn) ) \n",
    "    df_cool['y'] = df_cool['delta_temp']\n",
    "    df_cool['delta_temp'] = df_cool['delta_temp']* l1[3]\n",
    "    df_cool['initial_temp'] = max_heat_temp\n",
    "    df_cool['temp'] = df_cool['initial_temp'] - df_cool['delta_temp'] \n",
    "\n",
    "    df_cool['temp'] = df_cool['delta_temp']#* l1[3]\n",
    "    \n",
    "    df_cool_check = df_cool[df_cool['temp']<= limit_temp]\n",
    "    cooled_upto = df_cool_check['temp'].max()\n",
    "    #df_cool = df_cool[df_cool['temp']>=limit_temp]\n",
    "    initial_temp = cooled_upto\n",
    "    \n",
    "    this_df = pd.concat([df,df_cool])\n",
    "    temp_profile_df = pd.concat([temp_profile_df,this_df])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# df[\"time_index_new\"] = df[\"time_index\"]/60/1800\n",
    "# df['delta_temp'] = df['time_index_new'].apply(lambda a:evaluate_eqn(a,temp_eqn = cool_eqn) ) \n",
    "# #\n",
    "# xs = np.arange(10)\n",
    "# ys = 2*xs + 1\n",
    "\n",
    "# interp_func = interp1d(xs, ys)\n",
    "\n",
    "# newarr = interp_func(np.arange(2.1, 3, 0.1))\n",
    "\n",
    "# print(newarr)\n",
    "# eval(Y, globals, locals)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_eqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df#[df['time_index']<=25*60]\n",
    "df_cool['delta_temp'].max()\n",
    "df_cool\n",
    "temp_profile_df\n",
    "df_cool\n",
    "#df#[df['time_index']==1499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_profile_df[temp_profile_df['epoch']=='1_cool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"math.exp(x)\"\n",
    "x = 1\n",
    "eval(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
