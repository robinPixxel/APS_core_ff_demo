{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# with open('1_input_data/config.json', 'r') as file:\n",
    "#         config = json.load(file)\n",
    "# image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "\n",
    "# image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "# image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "# image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "# image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "# image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "# image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "# base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "\n",
    "# image_opportunity_df['base_time'] = base_time_stamp\n",
    "# image_opportunity_df['req_date'] = image_opportunity_df[['base_time','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)#pd.to_datetime(image_opportunity_df['base_time']) + pd.to_timedelta(image_opportunity_df['OpportunityStartOffset'])\n",
    "# image_opportunity_df[['req_date','OpportunityStartTime','base_time','OpportunityStartOffset']]\n",
    "\n",
    "# image_opportunity_df['x'] = image_opportunity_df[['Priority','StripID']].apply(lambda a: a['Priority'] if a['Priority']<=0 else a['StripID'],axis=1)\n",
    "# image_opportunity_df[['Priority','StripID','x']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa']\n",
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa', '../']\n"
     ]
    }
   ],
   "source": [
    "#!pip list\n",
    "import os \n",
    "os.getcwd()\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 66, 'c': 43, 'd': 55, 'ww': 222}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_opportunity_df.columns\n",
    "result_dict = {'a':2,'b':66,'c':43}\n",
    "result_dict.update({'d':55,'ww':222})\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "# from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "# from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "# from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "# from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "# from model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "# from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "# from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "# from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "# from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "# from result_interpret import interpret_result\n",
    "# from utils import *\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from APS_Python_core.model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from APS_Python_core.postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from APS_Python_core.model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "from APS_Python_core.postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from APS_Python_core.model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from APS_Python_core.postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from APS_Python_core.result_interpret import interpret_result\n",
    "from APS_Python_core.utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_time'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    # data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    #GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df_original = GS_pass_df.copy()\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    #image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    #image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.6))] + [0 for i in range(int(1.5*3600*0.4))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = GS_pass_df_original\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "def get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    #APS_Python_core/src/APS_Python_core/1_input_data/config.json\n",
    "    #with open('APS_Python_core/src/APS_Python_core/1_input_data/config.json', 'r') as file:\n",
    "    #with open('../1_input_data/config.json', 'r') as file:\n",
    "        #config = json.load(file)\n",
    "    original_image_opportunity_df = image_opportunity_df.copy()\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "    config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "    config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    # gs_pass_result_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    # img_capture_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    # APS_success_metric_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/APS_success_metric.csv\",index = None)\n",
    "    # downlink_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = original_image_opportunity_df\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        #v.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/\"+k+\".csv\",index = None)\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_gsPass_result = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout'][['SatID','start_time','end_time','gsID','base_time']]\n",
    "    only_gsPass_result['start_time'] = only_gsPass_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_gsPass_result['end_time'] = only_gsPass_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result,\\\n",
    "                  \"only_gsPass_result\":only_gsPass_result,\\\n",
    "                  \"combined_result\":img_capture_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "    #return result_dict\n",
    "    return result_dict\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_solving\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [1e+00, 1e+09]\n",
      "  Cost   [1e+00, 1e+00]\n",
      "  Bound  [1e+00, 1e+00]\n",
      "  RHS    [9e+04, 1e+09]\n",
      "Presolving model\n",
      "335 rows, 250 cols, 750 nonzeros  0s\n",
      "169 rows, 167 cols, 418 nonzeros  0s\n",
      "77 rows, 75 cols, 188 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   77 rows\n",
      "   75 cols (37 binary, 0 integer, 0 implied int., 38 continuous)\n",
      "   188 nonzeros\n",
      "MIP-Timing:      0.0018 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -inf            inf                  inf        0      0      0         0     0.0s\n",
      " R       0       0         0   0.00%   -3001279.839525 -34044          8715.88%        0      0      0        76     0.0s\n",
      " C       0       0         0   0.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "         1       0         1 100.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -45431\n",
      "  Dual bound        -45431\n",
      "  Gap               0% (tolerance: 0.01%)\n",
      "  P-D integral      0.0395521870219\n",
      "  Solution status   feasible\n",
      "                    -45431 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    0 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            0.00 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 0\n",
      "  Nodes             1\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     152 (total)\n",
      "                    0 (strong br.)\n",
      "                    76 (separation)\n",
      "                    0 (heuristics)\n",
      "status= Optimal\n",
      "image_capture_plan_starting\n",
      "===========\n",
      "===========\n",
      "===========\n",
      "len_before_eclipse_transition_divide= 437\n",
      "len_after_eclipse_transition_divide= 517\n",
      "Before_Ambiguous_evnet_transition_divide = 517\n",
      "Empty DataFrame\n",
      "Columns: [SatID, encoded_stripId, start_time, end_time, Eclipse, TW_index, gsID, till_now_max, prev_max, global_TW, Memory_global_TW_index, concat_sat_MGWI, EcStEnd_list, len_EcStEnd_list, new_eclipse, new_start_time, new_end_time]\n",
      "Index: []\n",
      "After_len_of_ambiguous_evnet_transition_divide = 517\n",
      "[1 2 3]\n",
      "len_power_based_memory_based= 372 len_power_based_memory_based= 145\n",
      "final_len_power_based= 517\n",
      "GS_Pass_time_objective\n",
      "only gs check  FF01 1.0 [1.0]\n",
      "only gs check  FF02 1.0 [1.0]\n",
      "only gs check  FF03 1.0 [1.0]\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [8e-01, 1e+06]\n",
      "  Cost   [1e+00, 7e+02]\n",
      "  Bound  [1e+00, 7e+08]\n",
      "  RHS    [1e+00, 2e+08]\n",
      "Presolving model\n",
      "14869 rows, 4981 cols, 42733 nonzeros  0s\n",
      "8647 rows, 4364 cols, 26526 nonzeros  0s\n",
      "8647 rows, 4364 cols, 23724 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   8647 rows\n",
      "   4364 cols (3155 binary, 0 integer, 0 implied int., 1209 continuous)\n",
      "   23724 nonzeros\n",
      "MIP-Timing:       0.038 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -141310.402767  inf                  inf        0      0      0         0     0.0s\n",
      "         0       0         0   0.00%   -91644.261909   inf                  inf        0      0      3      4364     0.1s\n",
      " C       0       0         0   0.00%   -86325.826005   -35885.942577    140.56%       72     49     19      4592     0.6s\n",
      " L       0       0         0   0.00%   -65344.179766   -45802.33852      42.67%      802    316     19      8415     2.7s\n",
      " L       0       0         0   0.00%   -65344.179766   -46057.293533     41.88%      755    219     19      9213     3.0s\n",
      " T       0       0         0   0.00%   -65344.179766   -47446.812399     37.72%      755    219     19      9839     3.9s\n",
      " L     250     244         1   0.00%   -65344.179766   -47666.969416     37.08%     1118    318     19     12098     4.5s\n",
      " L     870     820        21   0.00%   -62963.930934   -47751.850114     31.86%     1413    277    907     20350     7.8s\n",
      " L    1091    1031        26   0.00%   -62698.63098    -47801.324623     31.17%     1527    175   1082     27640     9.9s\n",
      " L    1104    1014        34   0.00%   -62698.63098    -48283.582431     29.85%     1584    185   1360     38470    12.8s\n",
      "      1330    1081        89   0.10%   -62681.915944   -48283.582431     29.82%     1795    262   4804     72645    18.3s\n",
      "      1612    1104       191   0.12%   -62681.915944   -48283.582431     29.82%     1619    117   8257     98749    23.4s\n",
      "      1964    1145       334   0.13%   -62681.915944   -48283.582431     29.82%     1704     71   9335    125273    28.4s\n",
      "      2314    1154       504   0.13%   -62681.915944   -48283.582431     29.82%     1798     50   9560    152564    33.8s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 4434 rows, 1559 cols (353 bin., 0 int., 0 impl., 1206 cont.), and 12498 nonzeros\n",
      "\n",
      "      2467       0         0   0.00%   -49179.53302    -48283.582431      1.86%      136      0      0    160024    35.7s\n",
      "      2467       0         0   0.00%   -49179.53302    -48283.582431      1.86%      136     99      2    160417    35.8s\n",
      "      2468       0         1 100.00%   -48283.582431   -48283.582431      0.00%      315    123      2    160475    35.8s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -48283.5824309\n",
      "  Dual bound        -48283.5824309\n",
      "  Gap               0%\n",
      "  P-D integral      12.4491126961\n",
      "  Solution status   feasible\n",
      "                    -48283.5824309 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    8.94839757848e-14 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            35.78 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 6\n",
      "  Nodes             2468\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     160475 (total)\n",
      "                    0 (strong br.)\n",
      "                    8518 (separation)\n",
      "                    103847 (heuristics)\n",
      "status= Optimal\n",
      "Downlink_plan_starting\n"
     ]
    }
   ],
   "source": [
    "GS_pass_df = GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "image_opportunity_df = pd.read_csv(\"1_input_data/Imaging_new (1) copy.csv\")\n",
    "image_downlink_df = pd.read_csv(\"1_input_data/APS_imageTable_TV1.csv\")\n",
    "with open('1_input_data/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "result_dict = get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(image_opportunity_df['Priority'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['only_img_capture_result']['StripID'].nunique(),len(result_dict['only_gsPass_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture_plan_data_input['']\n",
    "#-48283.5824309\n",
    "#-48271\n",
    "len(result_dict['interpret_selected_oppr_conflict_comparision_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['interpret_selected_oppr_conflict_comparision_df']#['conflic_strip_flag_named'].nunique()##['interpret_selected_oppr_conflict_comparision_df']#['interpret_extracted_raw_file_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlink_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 'extracted_raw_file_df', 'selected_oppr_conflict_comparision_df', 'KPI_df'\n",
    "df111 = result_dict['extracted_raw_file_df']#.columns\n",
    "#interpret_result_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = img_capture_result[img_capture_result['operation']=='Imaging']#['StripID']#.nunique()\n",
    "Z = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout']\n",
    "Z['duration'] = Z['end_time']- Z['start_time']\n",
    "#Y[Y['StripID']=='']\n",
    "\n",
    "#Y[Y['encoded_strip_id']=='Order 1 - Strip 0_Area 0']\n",
    "Y['StripID'].nunique(),len(Y),len(Z),Z['duration'].sum(),gs_pass_result_df['duration'].sum(),len(gs_pass_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['SatID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = Y[Y['SatID']=='FF03']\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "CR = RZO[RZO['SatID']=='FF03']\n",
    "pd.concat([CI,CR]).sort_values(by='start_time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Order 1 - Strip 0_Area 0'\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "RZO[RZO['SatID']=='FF01']\n",
    "RZO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RZ = capture_plan_data_input['dedicated_readout_df']\n",
    "len(RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF02'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF03'].sort_values(by='start_time'))\n",
    "RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_capture_result['operation'].unique()\n",
    "\n",
    "[ item for item in capture_plan_data_input['Memory_NoimageGs_TW_list'] if item[2]=='FF01']\n",
    "#capture_plan_data_input['dedicatedReadoutTWlist__concat_sat_memoryTWindex']['FF01_136.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_eclipse_data.csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eclipse_df[eclipse_df['eclipse']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_df.sort_values(by='time_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SatID,start_time,end_time,eclipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['GS_pass_df', 'image_opportunity_df', 'image_downlink_df', 'eclipse_df_dict', 'config']\n",
    "#input_dict['eclipse_df_dict']['FF02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['readout_memory_capacity__s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['imgery_sat_id_list']#.keys()\n",
    "[s+'_'+str(n) for s in capture_plan_data_input['imgery_sat_id_list']+capture_plan_data_input['only_gs_sat_id_list'] \\\n",
    " if s in capture_plan_data_input['dedicatedReadoutTWIndex__sat'].keys() for n in capture_plan_data_input['dedicatedReadoutTWIndex__sat'][s]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "\n",
    "#eclipse_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['assured_tasking_based_on_input_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['cs1j2k2Domainlist__cs1j1k1']['FF01_Order 1 - Strip 0_Area 0_1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['GS_Pass_time_objective'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(img_capture_result[img_capture_result['operation']=='Imaging'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195e0eaf50104e28ab34090001e82265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='\\n<div id=\"ifr-pyg-000628e85fff47613XPOfjlN6kG4qK7a\" style=\"height: auto\">\\n    <hea…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    window.addEventListener(\"message\", function(event) {\n",
       "        const backgroundMap = {\n",
       "            \"dark\": \"hsl(240 10% 3.9%)\",\n",
       "            \"light\": \"hsl(0 0 100%)\",\n",
       "        };\n",
       "        const colorMap = {\n",
       "            \"dark\": \"hsl(0 0% 98%)\",\n",
       "            \"light\": \"hsl(240 10% 3.9%)\",\n",
       "        };\n",
       "        if (event.data.action === \"changeAppearance\" && event.data.gid === \"000628e85fff47613XPOfjlN6kG4qK7a\") {\n",
       "            var iframe = document.getElementById(\"gwalker-000628e85fff47613XPOfjlN6kG4qK7a\");\n",
       "            iframe.style.background  = backgroundMap[event.data.appearance];\n",
       "            iframe.style.color = colorMap[event.data.appearance];\n",
       "        }\n",
       "    });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pygwalker.api.pygwalker.PygWalker at 0x319ea1430>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "memory_plot_df = result_dict['combined_result']\n",
    "\n",
    "#memory_plot_df.columns\n",
    "#memory_plot_df[['SatID','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','base_time','OpportunityStartOffset','OpportunityEndOffset#']]\n",
    "memory_plot_df = memory_plot_df[['SatID','start_time','end_time','operation','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','base_time']]\n",
    "memory_plot_df.sort_values(by=['SatID','start_time'],inplace=True)\n",
    "\n",
    "memory_plot_df['till_now_max'] = memory_plot_df.groupby('SatID')['end_time'].cummax()\n",
    "memory_plot_df['prev_max'] = memory_plot_df.groupby('SatID')['till_now_max'].shift(1)\n",
    "\n",
    "memory_plot_df1 = memory_plot_df[memory_plot_df['start_time'] > memory_plot_df['prev_max'] + 1] \n",
    "memory_plot_df1['start_time1'] = memory_plot_df1['prev_max'] + 1 #TODO1 +1 is okay ?\n",
    "memory_plot_df1['end_time1'] = memory_plot_df1['start_time'] - 1\n",
    "memory_plot_df1['operation'] = 'idle'\n",
    "\n",
    "memory_plot_df1 = memory_plot_df1[['SatID','start_time1','end_time1','operation','base_time']]\n",
    "\n",
    "#memory_plot_df1 = memory_plot_df1.drop(['start_time', 'end_time','till_now_max','prev_max'], axis=1)\n",
    "memory_plot_df1.rename(columns={'start_time1':'start_time','end_time1':'end_time'},inplace=True)\n",
    "#imgGS_union_df1 ==> contains TW without img and without gs pass  table without eclipse divide\n",
    "final_memory_plot_df = pd.concat([memory_plot_df,memory_plot_df1])\n",
    "final_memory_plot_df.sort_values(by=['SatID','start_time'],inplace=True)\n",
    "\n",
    "final_memory_plot_df['camera_memory_value_endofTW'] = final_memory_plot_df['camera_memory_value_endofTW'].ffill()\n",
    "final_memory_plot_df['delta_camera_memory_value_in_this_TW'] = final_memory_plot_df['delta_camera_memory_value_in_this_TW'].fillna(0)\n",
    "final_memory_plot_df['start_time'] = final_memory_plot_df[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "final_memory_plot_df['end_time'] = final_memory_plot_df[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "final_memory_plot_df = final_memory_plot_df[['SatID','start_time','end_time','camera_memory_value_endofTW','delta_camera_memory_value_in_this_TW','operation','base_time']]\n",
    "sat_list = final_memory_plot_df['SatID'].unique()\n",
    "#sat_list = ['FF01']\n",
    "\n",
    "\n",
    "colors = {\n",
    "        \"Imaging\": \"blue\",\n",
    "        \"downlinking_from_Readout\": \"green\",\n",
    "        \"Readout\": \"red\",\n",
    "        \"idle\": \"gray\"\n",
    "    }\n",
    "\n",
    "\n",
    "for s in sat_list:\n",
    "    this_plot_df = final_memory_plot_df[final_memory_plot_df['SatID']==s]\n",
    "    start_time_list = this_plot_df['start_time'].to_list()\n",
    "    end_time_list = this_plot_df['end_time'].to_list()\n",
    "    operation_list = this_plot_df['operation'].to_list()\n",
    "\n",
    "    camera_memory_value_endofTW_list = this_plot_df['camera_memory_value_endofTW'].to_list()\n",
    "    this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_endofTW'].shift(1)\n",
    "\n",
    "    first_row_list = this_plot_df.values.tolist()[0]\n",
    "    first_row_operation = first_row_list[5]\n",
    "    first_row_memory_val = first_row_list[3]\n",
    "    ortherwise_memory_val =  first_row_list[3] - first_row_list[4]\n",
    "    if first_row_operation not in['Imaging','Readout']:\n",
    "        this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_startofTW_list'].fillna(first_row_memory_val)\n",
    "    else:\n",
    "        this_plot_df['camera_memory_value_startofTW_list'] = this_plot_df['camera_memory_value_startofTW_list'].fillna(ortherwise_memory_val)\n",
    "\n",
    "    camera_memory_value_StartofTW_list = this_plot_df['camera_memory_value_startofTW_list'].to_list()\n",
    "    \n",
    "    time_list = list(itertools.chain.from_iterable(zip(start_time_list,end_time_list )))\n",
    "    operation_list = list(itertools.chain.from_iterable(zip(operation_list,operation_list )))\n",
    "    camera_memory_value_list = list(itertools.chain.from_iterable(zip(camera_memory_value_StartofTW_list,camera_memory_value_endofTW_list )))\n",
    "    df = pd.DataFrame({'time':time_list,'operation':operation_list,'memory':camera_memory_value_list})\n",
    "\n",
    "    this_list = this_plot_df.values.tolist()\n",
    "    # Plot using Plotly Express\n",
    "    # fig = px.line(this_plot_df, x='start_time', y='camera_memory_value_endofTW', color='operation', line_group='operation',\n",
    "    #             title=\"Memory Usage of Different Operations\")\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #                         x=time_list,\n",
    "    #                         y=camera_memory_value_list,\n",
    "    #                         mode='lines+markers+text',\n",
    "    #                         #name=operation_list,\n",
    "    #                         #line=dict(color=colors.get(operation, 'black'), width=2),\n",
    "    #                         #text=[f\"({start_time}, {current_memory:.2f})\", f\"({end_time}, {end_memory:.2f})\"],\n",
    "    #                         #textposition=text_position\n",
    "    #                     ))\n",
    "    \n",
    "    # fig.show()\n",
    "    \n",
    "    for item in this_list:\n",
    "            start_time = item[1]\n",
    "            end_time = item[2]\n",
    "            current_memory = item[6]\n",
    "            end_memory = item[3]\n",
    "            operation = item[5]\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                            x=[start_time, end_time],\n",
    "                            y=[current_memory, end_memory],\n",
    "                            mode='lines+markers+text',\n",
    "                            name=operation+'_'+s,\n",
    "                            line=dict(color=colors.get(operation, 'black'), width=2),\n",
    "                            #text=[f\"({start_time}, {current_memory:.2f})\", f\"({end_time}, {end_memory:.2f})\"],\n",
    "                            #textposition=text_position\n",
    "                        ))\n",
    "\n",
    "    #     # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Memory Profile Over Time\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Memory\",\n",
    "        legend_title=\"Operations\",\n",
    "          )\n",
    "    # names = set()\n",
    "    # fig.for_each_trace(\n",
    "    #     lambda trace:\n",
    "    #         trace.update(showlegend=False)\n",
    "    #         if (trace.name in names) else names.add(trace.name))\n",
    "\n",
    "    # Save figure as HTML\n",
    "    html_filename = \"memory_profile_.html\"\n",
    "    fig.write_html('5_output_data'+'/'+html_filename)\n",
    "\n",
    "# final_memory_plot_df[final_memory_plot_df['SatID']=='FF01'].sort_values(by='start_time')\n",
    "\n",
    "import pygwalker as pyg\n",
    "pyg.walk(final_memory_plot_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pygwalker:\n",
      "\n",
      "NAME\n",
      "    pygwalker\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _constants\n",
      "    _typing\n",
      "    api (package)\n",
      "    communications (package)\n",
      "    data_parsers (package)\n",
      "    errors\n",
      "    services (package)\n",
      "    utils (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        pygwalker.services.global_var.GlobalVarManager\n",
      "    pydantic.main.BaseModel(builtins.object)\n",
      "        pygwalker.data_parsers.base.FieldSpec\n",
      "\n",
      "    class FieldSpec(pydantic.main.BaseModel)\n",
      "     |  FieldSpec(*, fname: str, semantic_type: Literal['?', 'nominal', 'ordinal', 'temporal', 'quantitative'] = '?', analytic_type: Literal['?', 'dimension', 'measure'] = '?', display_as: str = None) -> None\n",
      "     |\n",
      "     |  Field specification.\n",
      "     |\n",
      "     |  Args:\n",
      "     |  - fname: str. The field name.\n",
      "     |  - semantic_type: '?' | 'nominal' | 'ordinal' | 'temporal' | 'quantitative'. default to '?'.\n",
      "     |  - analytic_type: '?' | 'dimension' | 'measure'. default to '?'.\n",
      "     |  - display_as: str. The field name displayed. None means using the original column name.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      FieldSpec\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'analytic_type': typing.Literal['?', 'dimension', '...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'pygwalker.data_parsers.base...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = {'Any': <pydantic._internal._model_con...\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"FieldSpec\", validator=...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, fname: str, semantic_type: Litera...mea...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  model_fields = {'analytic_type': FieldInfo(annotation=Literal['?', 'di...\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self: 'Model') -> 'Model'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self: 'Model', memo: 'dict[int, Any] | None' = None) -> 'Model'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(__pydantic_self__, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `__init__` uses `__pydantic_self__` instead of the more common `self` for the first arg to\n",
      "     |      allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self: 'Model', *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'typing.Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Model'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```py\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping\n",
      "     |              specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping\n",
      "     |              specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values\n",
      "     |              in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'typing.Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'typing.Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self: 'Model', *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Model'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.5/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: 'bool' = True) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.5/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the dictionary will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the dictionary may contain any Python objects.\n",
      "     |          include: A list of fields to include in the output.\n",
      "     |          exclude: A list of fields to exclude from the output.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value from the output.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None` from the output.\n",
      "     |          round_trip: Whether to enable serialization and deserialization round-trip support.\n",
      "     |          warnings: Whether to log warnings when invalid fields are encountered.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx' = None, exclude: 'IncEx' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: 'bool' = True) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.5/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output. Can take either a string or set of strings.\n",
      "     |          exclude: Field(s) to exclude from the JSON output. Can take either a string or set of strings.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that have the default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: Whether to use serialization/deserialization between JSON and class instance.\n",
      "     |          warnings: Whether to show any warnings that occurred during serialization.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(_BaseModel__source: 'type[BaseModel]', _BaseModel__handler: 'GetCoreSchemaHandler') -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          __source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          __handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(_BaseModel__core_schema: 'CoreSchema', _BaseModel__handler: 'GetJsonSchemaHandler') -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          __core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          __handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: The set of field names accepted for the Model instance.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'dict[str, Any] | None' = None) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to raise an exception on invalid fields.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'dict[str, Any] | None' = None) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.5/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValueError: If `json_data` is not a JSON string.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'dict[str, Any] | None' = None) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object contains string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object contains string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'typing.Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Model' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields__\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get the computed fields of this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of computed field names and their corresponding `ComputedFieldInfo` objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class GlobalVarManager(builtins.object)\n",
      "     |  A class to manage global variables.\n",
      "     |\n",
      "     |  Class methods defined here:\n",
      "     |\n",
      "     |  get_env() -> Literal['Jupyter', 'Streamlit'] from builtins.type\n",
      "     |\n",
      "     |  set_component_url(url: str) from builtins.type\n",
      "     |\n",
      "     |  set_env(env: Literal['Jupyter', 'Streamlit']) from builtins.type\n",
      "     |\n",
      "     |  set_kanaries_api_host(api_host: str) from builtins.type\n",
      "     |\n",
      "     |  set_kanaries_api_key(api_key: str) from builtins.type\n",
      "     |\n",
      "     |  set_kanaries_main_host(main_host: str) from builtins.type\n",
      "     |\n",
      "     |  set_last_exported_dataframe(df: pandas.core.frame.DataFrame) from builtins.type\n",
      "     |\n",
      "     |  set_max_data_length(length: int) from builtins.type\n",
      "     |\n",
      "     |  set_privacy(privacy: Literal['offline', 'update-only', 'events']) from builtins.type\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  component_url = ''\n",
      "     |\n",
      "     |  env = None\n",
      "     |\n",
      "     |  kanaries_api_host = 'https://api.kanaries.net'\n",
      "     |\n",
      "     |  kanaries_api_key = ''\n",
      "     |\n",
      "     |  kanaries_main_host = 'https://kanaries.net'\n",
      "     |\n",
      "     |  last_exported_dataframe = None\n",
      "     |\n",
      "     |  max_data_length = 1000000\n",
      "     |\n",
      "     |  privacy = 'events'\n",
      "\n",
      "FUNCTIONS\n",
      "    component(dataset: Union[~DataFrame, pygwalker.data_parsers.database_parser.Connector, str], *, field_specs: Optional[List[pygwalker.data_parsers.base.FieldSpec]] = None, spec: str = '', spec_io_mode: Literal['r', 'rw'] = 'rw', theme_key: Literal['vega', 'g2', 'streamlit'] = 'vega', appearance: Literal['media', 'light', 'dark'] = 'media', show_cloud_tool: Optional[bool] = False, kernel_computation: Optional[bool] = None, kanaries_api_key: str = '', **kwargs) -> pygwalker.api.component.Component\n",
      "        Component class for creating a chain of components.\n",
      "\n",
      "        Args:\n",
      "            - dataset (pl.DataFrame | pd.DataFrame | Connector, optional): dataframe.\n",
      "\n",
      "        Kargs:\n",
      "            - field_specs (List[FieldSpec], optional): Specifications of some fields. They'll been automatically inferred from `df` if some fields are not specified.\n",
      "            - spec (str): chart config data. config id, json, remote file url\n",
      "            - spec_io_mode (ISpecIOMode): spec io mode, Default to \"r\", \"r\" for read, \"rw\" for read and write.\n",
      "            - theme_key ('vega' | 'g2' | 'streamlit'): theme type.\n",
      "            - appearance (Literal['media' | 'light' | 'dark']): 'media': auto detect OS theme.\n",
      "            - kernel_computation(bool): Whether to use kernel compute for datas, Default to None.\n",
      "            - kanaries_api_key (str): kanaries api key, Default to \"\".\n",
      "\n",
      "    render(dataset: Union[~DataFrame, pygwalker.data_parsers.database_parser.Connector, str], spec: str, *, theme_key: Literal['vega', 'g2', 'streamlit'] = 'g2', appearance: Literal['media', 'light', 'dark'] = 'media', kernel_computation: Optional[bool] = None, kanaries_api_key: str = '', **kwargs)\n",
      "        Args:\n",
      "            - dataset (pl.DataFrame | pd.DataFrame | Connector, optional): dataframe.\n",
      "            - spec (str): chart config data. config id, json, remote file url\n",
      "\n",
      "        Kargs:\n",
      "            - theme_key ('vega' | 'g2'): theme type.\n",
      "            - appearance (Literal['media' | 'light' | 'dark']): 'media': auto detect OS theme.\n",
      "            - kernel_computation(bool): Whether to use kernel compute for datas, Default to None.\n",
      "            - kanaries_api_key (str): kanaries api key, Default to \"\".\n",
      "\n",
      "    table(dataset: Union[~DataFrame, pygwalker.data_parsers.database_parser.Connector, str], *, theme_key: Literal['vega', 'g2', 'streamlit'] = 'g2', appearance: Literal['media', 'light', 'dark'] = 'media', kernel_computation: Optional[bool] = None, kanaries_api_key: str = '', **kwargs)\n",
      "        Args:\n",
      "            - dataset (pl.DataFrame | pd.DataFrame | Connector, optional): dataframe.\n",
      "\n",
      "        Kargs:\n",
      "            - theme_key ('vega' | 'g2'): theme type.\n",
      "            - appearance (Literal['media' | 'light' | 'dark']): 'media': auto detect OS theme.\n",
      "            - kernel_computation(bool): Whether to use kernel compute for datas, Default to None.\n",
      "            - kanaries_api_key (str): kanaries api key, Default to \"\".\n",
      "\n",
      "    to_html(df: ~DataFrame, gid: Union[int, str] = None, *, spec: str = '', field_specs: Optional[List[pygwalker.data_parsers.base.FieldSpec]] = None, theme_key: Literal['vega', 'g2', 'streamlit'] = 'g2', appearance: Literal['media', 'light', 'dark'] = 'media', default_tab: Literal['data', 'vis'] = 'vis', **kwargs) -> str\n",
      "        Generate embeddable HTML code of Graphic Walker with data of `df`.\n",
      "\n",
      "        Args:\n",
      "            - df (pl.DataFrame | pd.DataFrame, optional): dataframe.\n",
      "            - gid (Union[int, str], optional): GraphicWalker container div's id ('gwalker-{gid}')\n",
      "\n",
      "        Kargs:\n",
      "            - field_specs (List[FieldSpec], optional): Specifications of some fields. They'll been automatically inferred from `df` if some fields are not specified.\n",
      "            - spec (str): chart config data. config id, json, remote file url\n",
      "            - theme_key ('vega' | 'g2'): theme type.\n",
      "            - appearance ('media' | 'light' | 'dark'): 'media': auto detect OS theme.\n",
      "            - default_tab (Literal[\"data\", \"vis\"]): default tab to show. Default to \"vis\"\n",
      "\n",
      "    walk(dataset: Union[~DataFrame, pygwalker.data_parsers.database_parser.Connector, str], gid: Union[int, str] = None, *, env: Literal['Jupyter', 'JupyterWidget'] = 'JupyterWidget', field_specs: Optional[List[pygwalker.data_parsers.base.FieldSpec]] = None, theme_key: Literal['vega', 'g2', 'streamlit'] = 'g2', appearance: Literal['media', 'light', 'dark'] = 'media', spec: str = '', use_kernel_calc: Optional[bool] = None, kernel_computation: Optional[bool] = None, cloud_computation: bool = False, show_cloud_tool: bool = True, kanaries_api_key: str = '', default_tab: Literal['data', 'vis'] = 'vis', **kwargs)\n",
      "        Walk through pandas.DataFrame df with Graphic Walker\n",
      "\n",
      "        Args:\n",
      "            - dataset (pl.DataFrame | pd.DataFrame | Connector, optional): dataframe.\n",
      "            - gid (Union[int, str], optional): GraphicWalker container div's id ('gwalker-{gid}')\n",
      "\n",
      "        Kargs:\n",
      "            - env: (Literal['Jupyter' | 'JupyterWidget'], optional): The enviroment using pygwalker. Default as 'JupyterWidget'\n",
      "            - field_specs (List[FieldSpec], optional): Specifications of some fields. They'll been automatically inferred from `df` if some fields are not specified.\n",
      "            - theme_key ('vega' | 'g2' | 'streamlit'): theme type.\n",
      "            - appearance (Literal['media' | 'light' | 'dark']): 'media': auto detect OS theme.\n",
      "            - spec (str): chart config data. config id, json, remote file url\n",
      "            - use_kernel_calc(bool): Whether to use kernel compute for datas, Default to None, automatically determine whether to use kernel calculation.\n",
      "            - kanaries_api_key (str): kanaries api key, Default to \"\".\n",
      "            - default_tab (Literal[\"data\", \"vis\"]): default tab to show. Default to \"vis\"\n",
      "            - cloud_computation(bool): Whether to use cloud compute for datas, it upload your data to kanaries cloud. Default to False.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['walk', 'render', 'table', 'to_html', 'FieldSpec', 'GlobalV...\n",
      "    __hash__ = 'Pfg84d71'\n",
      "\n",
      "VERSION\n",
      "    0.4.9.13\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/lib/python3.12/site-packages/pygwalker/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import os\n",
    "from preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from result_interpret import interpret_result\n",
    "from utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_time'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    # data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 2  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = True\n",
    "    # #config['objective']['total_readout_memory'] = False\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    # data['total_priority_objective'] = obj_model.prob.objective.value()\n",
    "    #++++++++++++++++++++++++++  STEP 3  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    \n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.6))] + [0 for i in range(int(1.5*3600*0.4))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    with open('1_input_data/config.json', 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "        config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "        config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    try:\n",
    "        print(dfd)\n",
    "        downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    except:\n",
    "        print(\"downlink_schedule_has_some_error\")\n",
    "        downlink_result = pd.DataFrame()\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    gs_pass_result_df.to_csv(\"5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    img_capture_result.to_csv(\"5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    APS_success_metric_df.to_csv(\"5_output_data/APS_success_metric.csv\",index = None)\n",
    "    downlink_result.to_csv(\"5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        v.to_csv(\"5_output_data/\"+k+\".csv\",index = None)\n",
    "\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
