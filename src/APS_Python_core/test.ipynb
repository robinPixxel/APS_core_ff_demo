{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# with open('1_input_data/config.json', 'r') as file:\n",
    "#         config = json.load(file)\n",
    "# image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "\n",
    "# image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "# image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "# image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "# image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "# image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "# image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "# base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "\n",
    "# image_opportunity_df['base_time'] = base_time_stamp\n",
    "# image_opportunity_df['req_date'] = image_opportunity_df[['base_time','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)#pd.to_datetime(image_opportunity_df['base_time']) + pd.to_timedelta(image_opportunity_df['OpportunityStartOffset'])\n",
    "# image_opportunity_df[['req_date','OpportunityStartTime','base_time','OpportunityStartOffset']]\n",
    "\n",
    "# image_opportunity_df['x'] = image_opportunity_df[['Priority','StripID']].apply(lambda a: a['Priority'] if a['Priority']<=0 else a['StripID'],axis=1)\n",
    "# image_opportunity_df[['Priority','StripID','x']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa']\n",
      "['/Users/robin/Documents/Schedule_plan/git_APS_Python_core_develop/APS_Python_core/src/APS_Python_core', '/opt/anaconda3/lib/python312.zip', '/opt/anaconda3/lib/python3.12', '/opt/anaconda3/lib/python3.12/lib-dynload', '', '/opt/anaconda3/lib/python3.12/site-packages', '/opt/anaconda3/lib/python3.12/site-packages/aeosa', '../']\n"
     ]
    }
   ],
   "source": [
    "#!pip list\n",
    "import os \n",
    "os.getcwd()\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 66, 'c': 43, 'd': 55, 'ww': 222}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_opportunity_df.columns\n",
    "result_dict = {'a':2,'b':66,'c':43}\n",
    "result_dict.update({'d':55,'ww':222})\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "# from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "# from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "# from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "# from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "# from model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "# from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "# from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "# from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "# from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "# from result_interpret import interpret_result\n",
    "# from utils import *\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from APS_Python_core.model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from APS_Python_core.postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from APS_Python_core.model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "from APS_Python_core.postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from APS_Python_core.preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from APS_Python_core.model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from APS_Python_core.postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from APS_Python_core.result_interpret import interpret_result\n",
    "from APS_Python_core.utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_time'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    # data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 2  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = True\n",
    "    # #config['objective']['total_readout_memory'] = False\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    # data['total_priority_objective'] = obj_model.prob.objective.value()\n",
    "    #++++++++++++++++++++++++++  STEP 3  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    \n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    #GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df_original = GS_pass_df.copy()\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    #image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    #image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.6))] + [0 for i in range(int(1.5*3600*0.4))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = GS_pass_df_original\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "def get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df):\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    #APS_Python_core/src/APS_Python_core/1_input_data/config.json\n",
    "    #with open('APS_Python_core/src/APS_Python_core/1_input_data/config.json', 'r') as file:\n",
    "    #with open('../1_input_data/config.json', 'r') as file:\n",
    "        #config = json.load(file)\n",
    "    original_image_opportunity_df = image_opportunity_df.copy()\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "    config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "    config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    # gs_pass_result_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    # img_capture_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    # APS_success_metric_df.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/APS_success_metric.csv\",index = None)\n",
    "    # downlink_result.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = original_image_opportunity_df\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        #v.to_csv(\"APS_Python_core/src/APS_Python_core/5_output_data/\"+k+\".csv\",index = None)\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_gsPass_result = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout'][['SatID','start_time','end_time','gsID','base_time']]\n",
    "    only_gsPass_result['start_time'] = only_gsPass_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_gsPass_result['end_time'] = only_gsPass_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result,\\\n",
    "                  \"only_gsPass_result\":only_gsPass_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "    #return result_dict\n",
    "    return result_dict\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_solving\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [1e+00, 1e+09]\n",
      "  Cost   [1e+00, 1e+00]\n",
      "  Bound  [1e+00, 1e+00]\n",
      "  RHS    [9e+04, 1e+09]\n",
      "Presolving model\n",
      "335 rows, 250 cols, 750 nonzeros  0s\n",
      "169 rows, 167 cols, 418 nonzeros  0s\n",
      "77 rows, 75 cols, 188 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   77 rows\n",
      "   75 cols (37 binary, 0 integer, 0 implied int., 38 continuous)\n",
      "   188 nonzeros\n",
      "MIP-Timing:     0.00065 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -inf            inf                  inf        0      0      0         0     0.0s\n",
      " R       0       0         0   0.00%   -3001279.839525 -34044          8715.88%        0      0      0        76     0.0s\n",
      " C       0       0         0   0.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "         1       0         1 100.00%   -45431          -45431             0.00%       42     37      0       152     0.0s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -45431\n",
      "  Dual bound        -45431\n",
      "  Gap               0% (tolerance: 0.01%)\n",
      "  P-D integral      0.0351583112994\n",
      "  Solution status   feasible\n",
      "                    -45431 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    0 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            0.00 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 0\n",
      "  Nodes             1\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     152 (total)\n",
      "                    0 (strong br.)\n",
      "                    76 (separation)\n",
      "                    0 (heuristics)\n",
      "status= Optimal\n",
      "image_capture_plan_starting\n",
      "===========\n",
      "===========\n",
      "===========\n",
      "len_before_eclipse_transition_divide= 437\n",
      "len_after_eclipse_transition_divide= 517\n",
      "Before_Ambiguous_evnet_transition_divide = 517\n",
      "Empty DataFrame\n",
      "Columns: [SatID, encoded_stripId, start_time, end_time, Eclipse, TW_index, gsID, till_now_max, prev_max, global_TW, Memory_global_TW_index, concat_sat_MGWI, EcStEnd_list, len_EcStEnd_list, new_eclipse, new_start_time, new_end_time]\n",
      "Index: []\n",
      "After_len_of_ambiguous_evnet_transition_divide = 517\n",
      "[1 2 3]\n",
      "len_power_based_memory_based= 372 len_power_based_memory_based= 145\n",
      "final_len_power_based= 517\n",
      "GS_Pass_time_objective\n",
      "only gs check  FF01 1.0 [1.0]\n",
      "only gs check  FF02 1.0 [1.0]\n",
      "only gs check  FF03 1.0 [1.0]\n",
      "Running HiGHS 1.8.1 (git hash: 4a7f24a): Copyright (c) 2024 HiGHS under MIT licence terms\n",
      "Coefficient ranges:\n",
      "  Matrix [8e-01, 1e+06]\n",
      "  Cost   [1e+00, 7e+02]\n",
      "  Bound  [1e+00, 7e+08]\n",
      "  RHS    [1e+00, 2e+08]\n",
      "Presolving model\n",
      "14869 rows, 4981 cols, 42733 nonzeros  0s\n",
      "8647 rows, 4364 cols, 26526 nonzeros  0s\n",
      "8647 rows, 4364 cols, 23724 nonzeros  0s\n",
      "\n",
      "Solving MIP model with:\n",
      "   8647 rows\n",
      "   4364 cols (3155 binary, 0 integer, 0 implied int., 1209 continuous)\n",
      "   23724 nonzeros\n",
      "MIP-Timing:        0.04 - starting analytic centre calculation\n",
      "\n",
      "Src: B => Branching; C => Central rounding; F => Feasibility pump; H => Heuristic; L => Sub-MIP;\n",
      "     P => Empty MIP; R => Randomized rounding; S => Solve LP; T => Evaluate node; U => Unbounded;\n",
      "     z => Trivial zero; l => Trivial lower; u => Trivial upper; p => Trivial point\n",
      "\n",
      "        Nodes      |    B&B Tree     |            Objective Bounds              |  Dynamic Constraints |       Work      \n",
      "Src  Proc. InQueue |  Leaves   Expl. | BestBound       BestSol              Gap |   Cuts   InLp Confl. | LpIters     Time\n",
      "\n",
      "         0       0         0   0.00%   -141310.402767  inf                  inf        0      0      0         0     0.0s\n",
      "         0       0         0   0.00%   -91644.261909   inf                  inf        0      0      3      4381     0.1s\n",
      " C       0       0         0   0.00%   -85553.622962   -35885.942577    138.40%       87     68     18      4724     0.4s\n",
      " L       0       0         0   0.00%   -70486.560883   -45842.991539     53.76%      558    271     18      6784     1.9s\n",
      " L       0       0         0   0.00%   -70486.560883   -46191.821811     52.60%      543    219     18      7576     2.2s\n",
      " T       0       0         0   0.00%   -70486.560883   -47508.00668      48.37%      543    219     18      8212     2.8s\n",
      " L     246     236         1   0.00%   -70486.560883   -48198.021315     46.24%      797    293     18      9864     3.2s\n",
      " L     680     653        10   0.00%   -68585.182753   -48205.111615     42.28%      836    270    449     15702     5.7s\n",
      "      1139     831       122   0.00%   -66943.992908   -48205.111615     38.87%     1108    180   5509     45503    11.2s\n",
      " L    1260     833       163   0.05%   -66899.806488   -48209.386268     38.77%     1250    198   6584     62614    14.9s\n",
      " L    1489     850       252   0.07%   -66077.404278   -48247.630688     36.95%     1380    312   8302     78645    17.3s\n",
      " L    1592     884       279   0.09%   -66025.265264   -48283.582431     36.74%     1443    159   8794     80627    18.0s\n",
      "      1942     920       420   0.10%   -65705.959792   -48283.582431     36.08%     1592    174   9539    110541    23.2s\n",
      "      2184     924       530   0.16%   -65468.468366   -48283.582431     35.59%     1886    160   8158    138341    28.2s\n",
      "\n",
      "Restarting search from the root node\n",
      "Model after restart has 4434 rows, 1559 cols (353 bin., 0 int., 0 impl., 1206 cont.), and 12498 nonzeros\n",
      "\n",
      "      2542       0         0   0.00%   -49179.53302    -48283.582431      1.86%      261      0      0    163556    32.7s\n",
      "      2542       0         0   0.00%   -49179.53302    -48283.582431      1.86%      261    160      2    164107    32.7s\n",
      "      2543       0         1 100.00%   -48283.582431   -48283.582431      0.00%      959    260      2    164790    32.9s\n",
      "\n",
      "Solving report\n",
      "  Status            Optimal\n",
      "  Primal bound      -48283.5824309\n",
      "  Dual bound        -48283.5824309\n",
      "  Gap               0%\n",
      "  P-D integral      13.569401127\n",
      "  Solution status   feasible\n",
      "                    -48283.5824309 (objective)\n",
      "                    0 (bound viol.)\n",
      "                    0 (int. viol.)\n",
      "                    0 (row viol.)\n",
      "  Timing            32.87 (total)\n",
      "                    0.00 (presolve)\n",
      "                    0.00 (solve)\n",
      "                    0.00 (postsolve)\n",
      "  Max sub-MIP depth 3\n",
      "  Nodes             2543\n",
      "  Repair LPs        0 (0 feasible; 0 iterations)\n",
      "  LP iterations     164790 (total)\n",
      "                    0 (strong br.)\n",
      "                    8916 (separation)\n",
      "                    103719 (heuristics)\n",
      "status= Optimal\n",
      "Downlink_plan_starting\n"
     ]
    }
   ],
   "source": [
    "GS_pass_df = GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "image_opportunity_df = pd.read_csv(\"1_input_data/Imaging_new (1) copy.csv\")\n",
    "image_downlink_df = pd.read_csv(\"1_input_data/APS_imageTable_TV1.csv\")\n",
    "with open('1_input_data/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "result_dict = get_schedule(config,GS_pass_df,image_opportunity_df,image_downlink_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(image_opportunity_df['Priority'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 83)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['only_img_capture_result']['StripID'].nunique(),len(result_dict['only_gsPass_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#capture_plan_data_input['']\n",
    "#-48283.5824309\n",
    "#-48271\n",
    "len(result_dict['interpret_selected_oppr_conflict_comparision_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['interpret_selected_oppr_conflict_comparision_df']#['conflic_strip_flag_named'].nunique()##['interpret_selected_oppr_conflict_comparision_df']#['interpret_extracted_raw_file_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlink_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 'extracted_raw_file_df', 'selected_oppr_conflict_comparision_df', 'KPI_df'\n",
    "df111 = result_dict['extracted_raw_file_df']#.columns\n",
    "#interpret_result_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = img_capture_result[img_capture_result['operation']=='Imaging']#['StripID']#.nunique()\n",
    "Z = img_capture_result[img_capture_result['operation']=='downlinking_from_Readout']\n",
    "Z['duration'] = Z['end_time']- Z['start_time']\n",
    "#Y[Y['StripID']=='']\n",
    "\n",
    "#Y[Y['encoded_strip_id']=='Order 1 - Strip 0_Area 0']\n",
    "Y['StripID'].nunique(),len(Y),len(Z),Z['duration'].sum(),gs_pass_result_df['duration'].sum(),len(gs_pass_result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['SatID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI = Y[Y['SatID']=='FF03']\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "CR = RZO[RZO['SatID']=='FF03']\n",
    "pd.concat([CI,CR]).sort_values(by='start_time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Order 1 - Strip 0_Area 0'\n",
    "RZO = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "RZO[RZO['SatID']=='FF01']\n",
    "RZO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RZ = capture_plan_data_input['dedicated_readout_df']\n",
    "len(RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF02'].sort_values(by='start_time')),len(RZ[RZ['SatID']=='FF03'].sort_values(by='start_time'))\n",
    "RZ[RZ['SatID']=='FF01'].sort_values(by='start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_capture_result['operation'].unique()\n",
    "\n",
    "[ item for item in capture_plan_data_input['Memory_NoimageGs_TW_list'] if item[2]=='FF01']\n",
    "#capture_plan_data_input['dedicatedReadoutTWlist__concat_sat_memoryTWindex']['FF01_136.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_eclipse_data.csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eclipse_df[eclipse_df['eclipse']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_df.sort_values(by='time_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SatID,start_time,end_time,eclipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['GS_pass_df', 'image_opportunity_df', 'image_downlink_df', 'eclipse_df_dict', 'config']\n",
    "#input_dict['eclipse_df_dict']['FF02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['readout_memory_capacity__s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['imgery_sat_id_list']#.keys()\n",
    "[s+'_'+str(n) for s in capture_plan_data_input['imgery_sat_id_list']+capture_plan_data_input['only_gs_sat_id_list'] \\\n",
    " if s in capture_plan_data_input['dedicatedReadoutTWIndex__sat'].keys() for n in capture_plan_data_input['dedicatedReadoutTWIndex__sat'][s]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "\n",
    "#eclipse_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['active_assured_strip_id_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['assured_tasking_based_on_input_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['cs1j2k2Domainlist__cs1j1k1']['FF01_Order 1 - Strip 0_Area 0_1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input['GS_Pass_time_objective'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(img_capture_result[img_capture_result['operation']=='Imaging'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_plan_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import os\n",
    "from preprocess_1.preprocess_GSpassSelecion import GSPassPreprocess\n",
    "from model_3.MILP_GSpassSelection_v3 import GSpassSelection\n",
    "from postprocess_4.postprocess_GSpassSelection import GSpasspostprocess\n",
    "\n",
    "from preprocess_1.preprocess_imageAqusuition_test import ImageAquisitionProcess #preprocess_imageAqusuition_test,preprocess_imageAquisition_v3_18112024\n",
    "from model_3.MILP_imageCapture_v3_17112024 import ImageCapturePlan # MILP_imageCapture_v2_16102024,MILP_imageCapture_v2_25102024 # MILP_imageCapture_v2_07112024 #MILP_imageCapture_v3_17112024\n",
    "from postprocess_4.image_capture_postprocess_V3_17112024 import ImagecapturePostProcess # image_capture_postprocess_V3_17112024# image_capture_postprocess_v2_18102024\n",
    "\n",
    "from preprocess_1.preprocess_downlink_WIP import DownlinkingPreProcess\n",
    "from model_3.MILP_downlink import ImageDownlinkPlan\n",
    "from postprocess_4.postprocess_downlink import ImageDownlinkPostProcess\n",
    "\n",
    "from result_interpret import interpret_result\n",
    "from utils import *\n",
    "\n",
    "# script_dir = os.path.abspath( os.path.dirname( __file__ ) )\n",
    "# print(\"script directory: \",script_dir)\n",
    "'''\n",
    "readout is happening after the last image and before entering the eclipse region.\n",
    "Any heating operation will start if the temp reaches around intial tempertaure.\n",
    "Higher number of Global priority is assumed to be Higher prior Important image.\n",
    "if end date of due date is less than 24 hrs from the reference time offset then it is going to assured tasking.\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM. Due date based assured tasking is the reason.\n",
    "Iniial Camera Memory is needed at the start of the any oppr imaging/gsPass whichever is first. \n",
    "Iniial Readout Memory is needed at the start of the readout oppr.\n",
    "Initial power is needed at start of the opportunity(imaging/gspass).For Now (since power constraint is not there for readout)\n",
    "Iniial thermal value is needed before the start of the oppr imaging/gsPass/readout according to device (for NCCms:readout , for camera detector: Imaging ,For XBT : gs Pass oppr).\n",
    "Eclipse Event should be starting from first oppr either gsPass/Imaging. Readout is happening after the first imaging so i guess not needed at readout.\n",
    "'''\n",
    "'''\n",
    "Need offset from when scheduling is started. Example if scheduling is  needed from 10:00AM to 11 PM . Then offset is needed from 10:00 AM or 09:59 AM.\n",
    "Iniial Memory,power and thermal value is needed at 10:00 AM\n",
    "'''\n",
    "def select_gs_pass_oppr(GS_pass_df,config):\n",
    "\n",
    "    obj_preprocess = GSPassPreprocess(GS_pass_df)\n",
    "    data = obj_preprocess.preprocess()\n",
    "\n",
    "    obj_model = GSpassSelection(data,config)\n",
    "    result,thermal_profile_gsPass = GSpasspostprocess(obj_model,data,config).get_gsPasses()# 21 seconds\n",
    "\n",
    "    try :\n",
    "        result['duration'] = result['end_time'] - result['start_time']\n",
    "        result = result[result['duration']> 0]\n",
    "    except:\n",
    "        print(\"model is not converged or infeasible or not solved\")\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def select_img_opprtunity(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config):\n",
    "\n",
    "    #basic flters\n",
    "    #image_opportunity_df = image_opportunity_df[image_opportunity_df['OpportunityEndOffset']<config['scheduled_Hrs']*3600]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['CloudCoverLimit']>image_opportunity_df['CloudCover']]\n",
    "    image_opportunity_df = image_opportunity_df[image_opportunity_df['OffNadirLimit']>image_opportunity_df['OffNadir']]\n",
    "\n",
    "    obj_preprocess = ImageAquisitionProcess(image_opportunity_df,gs_pass_result_df,eclipse_df_dict,config)\n",
    "    data = obj_preprocess.preprocess()\n",
    "    #print(data['cs1j2k2Domainlist__cs1j1k1'])\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 0  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    '''\n",
    "    hard code some data\n",
    "    '''\n",
    "    data['camera_memory_capacity__s'] = {s:v for s,v in data['camera_memory_capacity__s'].items() }\n",
    "    data['readout_memory_capacity__s'] = {s:v for s,v in data['readout_memory_capacity__s'].items() }\n",
    "    data['power_capacity__s']  = {s:720000000 for s,v in data['power_capacity__s'].items() }\n",
    "    data['initial_power_value__s']  = {s:v*0.3 for s,v in data['power_capacity__s'].items() }\n",
    "    #++++++++++++++++++++++++++  STEP 1  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    config['objective']['GS_Pass_time'] = True\n",
    "    config['objective']['total_priority'] = False\n",
    "    config['objective']['total_readout_memory'] = False\n",
    "    obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #Readout Schedule \n",
    "    # data['GS_Pass_time_objective'] = obj_model.prob.objective.value()\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    #++++++++++++++++++++++++++  STEP 2  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['GS_Pass_time'] = False\n",
    "    # config['objective']['total_priority'] = True\n",
    "    # #config['objective']['total_readout_memory'] = False\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "\n",
    "    # data['total_priority_objective'] = obj_model.prob.objective.value()\n",
    "    #++++++++++++++++++++++++++  STEP 3  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # config['objective']['total_priority'] = False\n",
    "    # config['objective']['total_readout_memory'] = True\n",
    "\n",
    "    # obj_model = ImageCapturePlan(data,config)\n",
    "    #++++++++++++++++++++++++++  PostProcess  +++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    post_obj = ImagecapturePostProcess(obj_model,data)\n",
    "    img_capture_result= post_obj.get_schedule()\n",
    "    #.isnull().sum()\n",
    "    return img_capture_result,data\n",
    "    \n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_aps_success_metric(img_capture_result,data):\n",
    "    after_aps_plan_df = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    criteria_list = ['total_opprtunities_ratio','GP_ratio','LP_ratio','conflictImg_gsPass']\n",
    "\n",
    "    total_conflict_images_list= [data['success_metric_before']['conflict_images']]\n",
    "    GP_before = data['success_metric_before']['original_Total_GP']\n",
    "    LP_before = data['success_metric_before']['original_Total_LP']\n",
    "    TOppr_before = data['success_metric_before']['total_opportunities'] \n",
    "    before_list = [TOppr_before,GP_before,LP_before,total_conflict_images_list]\n",
    "\n",
    "    GP_after= after_aps_plan_df['mean_global_priority'].sum()\n",
    "    LP_after = after_aps_plan_df['mean_local_priority'].sum()\n",
    "    TOppr_after= after_aps_plan_df['encoded_strip_id'].nunique()\n",
    "    fraction_conflict_images_list = [list(after_aps_plan_df[after_aps_plan_df['encoded_strip_id'].isin(total_conflict_images_list)]['encoded_strip_id'].unique())]\n",
    "    after_list = [TOppr_after,GP_after,LP_after,fraction_conflict_images_list]\n",
    "\n",
    "    APS_success_metric_df = pd.DataFrame({'criteria':criteria_list,'potential_input':before_list,'APS_selected':after_list})\n",
    "    #APS_success_metric_df['percentage'] = APS_success_metric_df['APS_selected'] / APS_success_metric_df['potential_input']\n",
    "    APS_success_metric_df1 = APS_success_metric_df[:-1]\n",
    "    APS_success_metric_df1['percentage'] = APS_success_metric_df1['APS_selected'] / APS_success_metric_df1['potential_input'] * 100\n",
    "    APS_success_metric_df2 = APS_success_metric_df[-1:]\n",
    "\n",
    "    APS_success_metric_df = pd.concat([APS_success_metric_df1,APS_success_metric_df2])\n",
    "\n",
    "    return APS_success_metric_df\n",
    "    \n",
    "\n",
    "def get_downlink_schedule(image_downlink_df,img_capture_result,config):\n",
    "\n",
    "    downlink_operation_list  = ['downlinking_from_camera','downlinking_from_Readout']\n",
    "    img_capture_result_downlink = img_capture_result[img_capture_result['operation'].isin(downlink_operation_list)]\n",
    "    DownlinkingPreProcessObj = DownlinkingPreProcess(image_downlink_df,img_capture_result_downlink,config)\n",
    "    data_downlink = DownlinkingPreProcessObj.preprocess()\n",
    "    \n",
    "    if config['downlink_schedule_OnlyJustsortImgID']:\n",
    "        downlink_result = pd.DataFrame(data_downlink['LP_DD_Priority_imgID'].items(),columns=['ImageID','computed_priority']).\\\n",
    "            sort_values(by='computed_priority',ascending=False)\n",
    "    else:\n",
    "        obj_downlink_model = ImageDownlinkPlan(data_downlink,config)\n",
    "        downlink_result = ImageDownlinkPostProcess(obj_downlink_model,data_downlink).get_schedule()\n",
    "        downlink_result = downlink_result[downlink_result['TileStripNo_downLoad']!=0]\n",
    "\n",
    "    return downlink_result\n",
    "    \n",
    "def schedule():\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_input_files(config):\n",
    "    # GS PASS\n",
    "    #GS_pass_df = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])#APS_gsPasses_TV1#GS_Passes_mock1#GS_Passes_live1#GS_Passes_new (1)\n",
    "    \n",
    "    GS_pass_df = pd.read_csv(\"1_input_data/GS_Passes_new (1).csv\")\n",
    "    GS_pass_df['SatID'] = GS_pass_df['SatID'].astype(str)\n",
    "    GS_pass_df['AOSOffset'] = GS_pass_df['AOSOffset'].astype(int)\n",
    "    GS_pass_df['LOSOffset'] = GS_pass_df['LOSOffset'].astype(int)\n",
    "\n",
    "    # image Opprtunity\n",
    "    image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])#Imaging_mock1#APS_imagingOpportunities_TV1#Imaging_live#Imaging_new (1)\n",
    "    # change made priority ulta\n",
    "    #image_opportunity_df['Priority'] = 1/image_opportunity_df['Priority']\n",
    "    \n",
    "    image_opportunity_df['SatID'] = image_opportunity_df['SatID'].astype(str)\n",
    "    image_opportunity_df['OpportunityStartOffset'] = image_opportunity_df['OpportunityStartOffset'].astype(int)\n",
    "    image_opportunity_df['OpportunityEndOffset'] = image_opportunity_df['OpportunityEndOffset'].astype(int)\n",
    "    image_opportunity_df_copy = image_opportunity_df.copy()\n",
    "    image_opportunity_df_copy['X'] = image_opportunity_df_copy[['OpportunityStartTime','OpportunityStartOffset']].apply(lambda a: pd.to_datetime(a['OpportunityStartTime']) - pd.DateOffset(seconds=a['OpportunityStartOffset']),axis=1)\n",
    "    image_opportunity_df_copy['Y'] = image_opportunity_df_copy[['OpportunityEndTime','OpportunityEndOffset']].apply(lambda a: pd.to_datetime(a['OpportunityEndTime']) - pd.DateOffset(seconds=a['OpportunityEndOffset']),axis=1)\n",
    "    base_time_stamp = image_opportunity_df_copy[\"X\"].to_list()[0]\n",
    "    config['base_time_stamp_downlink'] = base_time_stamp\n",
    "\n",
    "    #image Downlink\n",
    "    image_downlink_df = pd.read_csv(config[\"csv_file_path\"][\"image_downlink_file\"])\n",
    "    image_downlink_df['assured_downlink_flag'] = [0,0] +[0]*(len(image_downlink_df)-2)\n",
    "    image_downlink_df['delivery_type'] = 'standard_delivery' # expedited_delivery,super_expedited_delivery\n",
    "    union_list_of_sat = list(set(image_opportunity_df['SatID']).union(set(GS_pass_df['SatID'])).union(set(image_downlink_df['SatID'])))\n",
    "    hrs = config['scheduled_Hrs']\n",
    "\n",
    "    # get dummy eclipse data close to reality\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "        \n",
    "    min_time_index= min([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "    max_time_index= max([image_opportunity_df['OpportunityStartOffset'].min(),image_opportunity_df['OpportunityEndOffset'].max(),GS_pass_df['AOSOffset'].min(),GS_pass_df['LOSOffset'].max()])\n",
    "\n",
    "    hrs = (max_time_index - min_time_index)/3600\n",
    "    hrs = math.ceil(hrs)\n",
    "    while True:\n",
    "        hrs += 1\n",
    "        if hrs % 1.5==0:\n",
    "            break\n",
    "\n",
    "\n",
    "    in_orbit_eclipse_event = [1 for i in range(int(1.5*3600*0.6))] + [0 for i in range(int(1.5*3600*0.4))] #\n",
    "    eclipse_df  = pd.DataFrame({'time_index': [i for i in range(min_time_index,min_time_index+hrs*3600)] ,\"eclipse\" : in_orbit_eclipse_event*int(hrs/1.5)})\n",
    "    eclipse_df['SatID']= [union_list_of_sat] *len(eclipse_df)\n",
    "    eclipse_df = eclipse_df.explode('SatID')\n",
    "    eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    #eclipse_df = pd.DataFrame()\n",
    "    #for sat in satellite_list:\n",
    "        #this_eclipse_df = eclipse_event_df[eclipse_event_df['SatID']==sat]\n",
    "        #that_eclipse_df = get_eclipse_data(this_eclipse_df,config)\n",
    "        #eclipse_df = pd.concat([that_eclipse_df,eclipse_df])\n",
    "   #eclipse_df_dict = {s: eclipse_df[eclipse_df['SatID']==s] for s in eclipse_df['SatID'].unique()}\n",
    "\n",
    "    \n",
    "    # get dummy data for assured tasking\n",
    "    image_opportunity_df['encoded_stripId'] =   image_opportunity_df['StripID'].astype(str)+ '_' + image_opportunity_df['AoiID'].astype(str)\n",
    "    total_capture_list = list(image_opportunity_df['encoded_stripId'].unique())\n",
    "    no_of_list = len(total_capture_list)\n",
    "    assured_capture_df = pd.DataFrame({'encoded_stripId':total_capture_list,'assured_task':[0,0]+[0]*(no_of_list-2)})\n",
    "    image_opportunity_df = pd.merge(image_opportunity_df,assured_capture_df,on='encoded_stripId',how='left')\n",
    "    image_opportunity_df = image_opportunity_df.drop(columns=['encoded_stripId'])\n",
    "\n",
    "    # further processing eclipse data to align with gs pass where entire gs pass is assumed to be in eclipse region\n",
    "    gsPassInput_df_copy = pd.read_csv(config[\"csv_file_path\"][\"gs_pass_opportunity\"])\n",
    "    gsPassInput_df_copy['SatID'] = gsPassInput_df_copy['SatID'].astype(str)\n",
    "    gsPassInput_df_copy['AOSOffset'] = gsPassInput_df_copy['AOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['LOSOffset'] = gsPassInput_df_copy['LOSOffset'].astype(int)\n",
    "    gsPassInput_df_copy['list'] =  gsPassInput_df_copy[['AOSOffset','LOSOffset']].apply(lambda a : [i for i in range(a['AOSOffset'],a['LOSOffset']+1)],axis =1 )\n",
    "\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy[['SatID','list']]\n",
    "    gsPassInput_df_copy1 = gsPassInput_df_copy1.explode('list')\n",
    "    gsPassInput_df_grouped_copy1 = gsPassInput_df_copy1.groupby('SatID').agg(time_index_list = ('list',list)).reset_index()\n",
    "    gsPasstimeIndexList__s = dict(zip(gsPassInput_df_grouped_copy1['SatID'],gsPassInput_df_grouped_copy1['time_index_list']))\n",
    "    for k,v in eclipse_df_dict.items():\n",
    "        if k in gsPasstimeIndexList__s.keys():\n",
    "            this_time_index_list = gsPasstimeIndexList__s[k]\n",
    "            v.loc[v[\"time_index\"].isin(this_time_index_list), \"eclipse\"] = 1\n",
    "            eclipse_df_dict[k] = v\n",
    "\n",
    "    return {\n",
    "            'GS_pass_df':GS_pass_df,\\\n",
    "            'image_opportunity_df':image_opportunity_df,\\\n",
    "            'image_downlink_df':image_downlink_df,\\\n",
    "            \"eclipse_df_dict\": eclipse_df_dict,\n",
    "            \"config\":config\n",
    "            }\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    with open('1_input_data/config.json', 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    # if memory constraint False then thermal_constraint is also False\n",
    "        config['constraints']['thermal_constraint_readout'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_readout']\n",
    "        config['constraints']['thermal_constraint_imaging'] = config['constraints']['memory_constrant'] and config['constraints']['thermal_constraint_imaging']\n",
    "\n",
    "    #======================================================================================================================================================================================================\n",
    "    # read_input\n",
    "    input_dict = get_input_files(config)\n",
    "    config = input_dict['config']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #gs pass_selection\n",
    "    gs_pass_result_df = select_gs_pass_oppr(input_dict['GS_pass_df'],config)\n",
    "    gs_pass_result_df['Eclipse'] = 1 ## dummy\n",
    "    gs_pass_result_df['duration'] = gs_pass_result_df['end_time'] - gs_pass_result_df['start_time']\n",
    "    gs_pass_result_df = gs_pass_result_df[gs_pass_result_df['duration']> 0]\n",
    "    interpret_gs_pass_result_df_copy = gs_pass_result_df.copy()# this not the gsPass result as it is to be get filtered after due to other factors in image capture plan.It is just to get require info in interpret result.\n",
    "\n",
    "    print(\"image_capture_plan_starting\")\n",
    "    #======================================================================================================================================================================================================\n",
    "    #image_selection\n",
    "    img_capture_result,capture_plan_data_input= select_img_opprtunity(input_dict['image_opportunity_df'],gs_pass_result_df,input_dict['eclipse_df_dict'],config)\n",
    "    img_capture_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging']\n",
    "    #readout_result = img_capture_result[img_capture_result['operation']=='Readout']\n",
    "    interpret_img_capture_resul_copy = img_capture_result.copy()\n",
    "    #======================================================================================================================================================================================================\n",
    "    # get APS success metrics \n",
    "    APS_success_metric_df = get_aps_success_metric(img_capture_result,capture_plan_data_input)\n",
    "    #======================================================================================================================================================================================================\n",
    "    print(\"Downlink_plan_starting\")\n",
    "    try:\n",
    "        print(dfd)\n",
    "        downlink_result = get_downlink_schedule(input_dict['image_downlink_df'],img_capture_result,config)\n",
    "    except:\n",
    "        print(\"downlink_schedule_has_some_error\")\n",
    "        downlink_result = pd.DataFrame()\n",
    "    downlink_result['base_time'] = config['base_time_stamp_downlink']\n",
    "    #======================================================================================================================================================================================================\n",
    "    #img_capture_result[img_capture_result['download_from_']]\n",
    "    gs_pass_result_df.to_csv(\"5_output_data/gs_pass_result_df.csv\",index=None)\n",
    "    img_capture_result.to_csv(\"5_output_data/img_capture_schedule.csv\",index=None)\n",
    "    APS_success_metric_df.to_csv(\"5_output_data/APS_success_metric.csv\",index = None)\n",
    "    downlink_result.to_csv(\"5_output_data/downlink_result.csv\",index = None)\n",
    "\n",
    "    interpret_image_opportunity_df = pd.read_csv(config[\"csv_file_path\"][\"image_capture_opportunity\"])\n",
    "    interpret_result_dict = interpret_result(interpret_image_opportunity_df,interpret_gs_pass_result_df_copy,interpret_img_capture_resul_copy,config)\n",
    "    for k,v in interpret_result_dict.items():\n",
    "        v['base_time'] = config['base_time_stamp_downlink']\n",
    "        v.to_csv(\"5_output_data/\"+k+\".csv\",index = None)\n",
    "\n",
    "    \n",
    "    only_img_capture_result = img_capture_result[img_capture_result['operation']=='Imaging'][['SatID','start_time','end_time','AoiID','StripID','base_time']]\n",
    "    only_img_capture_result['start_time'] = only_img_capture_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_img_capture_result['end_time'] = only_img_capture_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    only_readout_result = img_capture_result[img_capture_result['operation']=='Readout'][['SatID','start_time','end_time','base_time']]\n",
    "    only_readout_result['start_time'] = only_readout_result[['start_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['start_time']),axis=1)\n",
    "    only_readout_result['end_time'] = only_readout_result[['end_time','base_time']].apply(lambda a: pd.to_datetime(a['base_time']) + pd.DateOffset(seconds=a['end_time']),axis=1)\n",
    "\n",
    "    result_dict = {\"only_readout_result\":only_readout_result,\\\n",
    "                  \"only_img_capture_result\":only_img_capture_result}\n",
    "    result_dict.update(interpret_result_dict)\n",
    "\n",
    "    #print(only_img_capture_result,only_readout_result,downlink_result)\n",
    "#config['constraints'] = ['Thermal_constraints']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
